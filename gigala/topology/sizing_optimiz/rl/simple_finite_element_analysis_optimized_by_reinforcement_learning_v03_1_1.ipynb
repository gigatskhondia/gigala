{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model tries to optimize stress-strain state (reduce displacements to acceptable level) by combination of simple finite element analysis and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, it is a simple form of material optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import time\n",
    "import random\n",
    "from stable_baselines3 import A2C, SAC,PPO\n",
    "import torch\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common import results_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Element Model of Axially Loaded Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details of model can be found at:\n",
    "# https://en.wikiversity.org/wiki/Introduction_to_finite_elements/Axial_bar_finite_element_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementStiffness(A, E, h):\n",
    "    s= A*E/h\n",
    "    return s*np.array([[1,-1],[-1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementLoad(node1, node2, a, h):\n",
    "\n",
    "    x1 = node1\n",
    "    x2 = node2\n",
    "\n",
    "    fe1 = a*x2/(2*h)*(x2**2-x1**2) - a/(3*h)*(x2**3-x1**3)\n",
    "    fe2 = -a*x1/(2*h)*(x2**2-x1**2) + a/(3*h)*(x2**3-x1**3)\n",
    "    return np.array([fe1,fe2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AxialBarFEM(A,E):\n",
    "    L = 1.0\n",
    "    a = 1.0\n",
    "    R = 1.0    \n",
    "    e = 3\n",
    "    h = L/e\n",
    "    n = e+1\n",
    "        \n",
    "    node=[]    \n",
    "    for i in range(n):\n",
    "        node.append(i*h)\n",
    "    node=np.array(node) \n",
    "        \n",
    "    elem=[]    \n",
    "    for i in range(e):\n",
    "        P=[i,i+1]\n",
    "        elem.append(P)\n",
    "    elem=np.array(elem)    \n",
    "      \n",
    "    K=np.zeros((n,n))   \n",
    "    F=np.zeros((n,1))  \n",
    "       \n",
    "    for i in range(e):\n",
    "        node1 = elem[i,0]\n",
    "        node2 = elem[i,1]\n",
    "        Ke = elementStiffness(A, E, h)\n",
    "        fe = elementLoad(node[node1],node[node2], a, h)\n",
    "        K[node1:node2+1,node1:node2+1] = K[node1:node2+1,node1:node2+1] + Ke\n",
    "        F[node1:node2+1] = F[node1:node2+1] + fe.reshape(2,1)\n",
    "         \n",
    "    F[n-1] = F[n-1] + 1.0\n",
    "   \n",
    "    bc_node=[0]\n",
    "    bc_val=[0]\n",
    "    # https://github.com/CALFEM/calfem-matlab/blob/master/fem/solveq.m\n",
    "    \n",
    "    \n",
    "    bc=np.array([bc_node, \n",
    "                bc_val]).T\n",
    "    nd, nd=K.shape\n",
    "    fdof=np.array([i for i in range(nd)]).T\n",
    "    d=np.zeros(shape=(len(fdof),))\n",
    "    Q=np.zeros(shape=(len(fdof),))\n",
    "\n",
    "    pdof=bc[:,0].astype(int)\n",
    "    dp=bc[:,1]\n",
    "    fdof=np.delete(fdof, pdof, 0)\n",
    "    s=np.linalg.lstsq(K[fdof,:][:,fdof], (F[fdof].T-np.dot(K[fdof,:][:,pdof],dp.T)).T, rcond=None)[0] \n",
    "    d[pdof]=dp\n",
    "    d[fdof]=s.reshape(-1,)\n",
    "    \n",
    "#     Q=np.dot(K,d).T-F \n",
    "    return d[-1],A,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: cross-sectional area and Young's modulus\n",
    "# Output: largest displacement at rightmost node at the point of external force application\n",
    "\n",
    "DIM=len(AxialBarFEM(0.5, 1))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prestep(action,A,E):\n",
    "    d=0.01\n",
    "    d1=0.005\n",
    "    if action==0:\n",
    "        return A-d, E\n",
    "    elif action==1:\n",
    "        return A-d,E-d1\n",
    "    elif action==2:\n",
    "        return A-d,E+d1\n",
    "    elif action==3:\n",
    "        return A,E\n",
    "    elif action==4:\n",
    "        return A+d,E\n",
    "    elif action==5:\n",
    "        return A+d, E+d1\n",
    "    elif action==6:\n",
    "        return A+d, E-d1\n",
    "    elif action==7:\n",
    "        return A, E+d1\n",
    "    else:\n",
    "        return A,E-d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_(obs_,obs):\n",
    "#     if obs_[1]>obs[1]: # use when minimizing cross-sectional area\n",
    "    if obs_[0]>obs[0]:  # use when minimizing displacement  \n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DISCRETE_ACTIONS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        \n",
    "        self.A=3*random.random()\n",
    "        self.E=2*random.random()\n",
    "        self.obs=list(AxialBarFEM(self.A, self.E))+[0]   \n",
    "        self.observation_space = spaces.Box(low=np.array([-np.inf for x in range(DIM)]),\n",
    "                                            high=np.array([np.inf for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.float64)\n",
    "        self.needs_reset = True\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        obs_=self.obs\n",
    "        self.A,self.E=prestep(action,self.A,self.E)\n",
    "        self.obs=list(AxialBarFEM(self.A,self.E))+[action]\n",
    "        reward=reward_(obs_,self.obs)\n",
    "                \n",
    "        done=False\n",
    "        if self.obs[0]<0.1: \n",
    "            done = True\n",
    "        \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "      \n",
    "        return np.array(self.obs), reward, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        self.A=3*random.random()\n",
    "        self.E=2*random.random()\n",
    "        self.obs=list(AxialBarFEM(self.A, self.E))+[0] \n",
    "        self.needs_reset = False\n",
    "        return np.array(self.obs)  \n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass    \n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = BarEnv()\n",
    "env = Monitor(env, log_dir)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2036.00\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 2036.00\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1457.50\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1243.33\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1097.25\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 980.40\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 948.00\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 916.86\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 919.38\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 897.44\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 877.00\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 840.17\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 788.57\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.40\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 747.76\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 725.05\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 714.80\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 708.57\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 709.65\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 708.42\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 705.69\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 706.44\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 690.55\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 688.20\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 678.28\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 673.76\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 667.17\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 661.32\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 662.84\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 652.55\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 651.17\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 651.29\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 650.18\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 645.56\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 641.81\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 639.22\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 639.36\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 636.43\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 636.98\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 639.31\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 640.04\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 634.79\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 631.32\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 630.43\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 627.00\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 621.31\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 610.94\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 609.50\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 608.74\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 608.31\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 607.41\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 606.15\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 604.58\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 604.87\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 600.97\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 599.94\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 601.21\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 600.39\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 598.56\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 597.31\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 598.66\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 598.52\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 599.10\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 600.21\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 600.76\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 601.99\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 601.88\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 602.52\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 602.00\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 587.16\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 585.17\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 585.34\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 585.56\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 586.22\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 582.85\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 583.36\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 584.95\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 586.84\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 594.28\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 596.15\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 597.23\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 597.23\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 603.27\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 605.31\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 607.30\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 606.96\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 606.90\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 608.24\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 608.24\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 613.89\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 622.97\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 633.64\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 633.64\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 641.13\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 649.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 99000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 655.77\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 655.77\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 666.50\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 669.16\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 678.28\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 681.62\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 681.62\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 691.14\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 691.14\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 706.58\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 709.58\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 719.90\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 724.22\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 725.26\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 725.26\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 734.49\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 742.38\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 742.38\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 747.99\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 760.49\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 763.21\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 763.21\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 763.21\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 782.96\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 787.59\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 791.07\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 794.18\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 797.39\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 797.50\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 797.50\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 797.50\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 820.96\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 827.50\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 831.49\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 831.49\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 843.35\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 846.51\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 856.39\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 858.02\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 858.02\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 858.02\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 883.16\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 888.21\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 892.10\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 899.83\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 905.01\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 914.25\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 919.54\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 919.54\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 919.54\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.82\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.82\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.82\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.82\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.82\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.82\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 999.88\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1004.41\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1004.62\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1004.62\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1015.77\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1095.40\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1100.37\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1104.76\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1104.76\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1104.76\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1104.76\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1135.26\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1135.26\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1152.68\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1152.68\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1152.68\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.42\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1241.13\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1239.12\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1240.96\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1241.31\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1240.48\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1238.99\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1233.62\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1236.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 193000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1229.37\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1228.91\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1229.13\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1224.40\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1222.18\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1219.49\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1214.89\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1210.57\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1205.59\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1202.29\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1198.04\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1200.45\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1196.39\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1193.06\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1188.58\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1186.77\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1186.20\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1177.40\n",
      "Num timesteps: 211000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1171.89\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1162.88\n",
      "Num timesteps: 213000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1148.44\n",
      "Num timesteps: 214000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1146.01\n",
      "Num timesteps: 215000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1135.88\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1125.25\n",
      "Num timesteps: 217000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1114.03\n",
      "Num timesteps: 218000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1101.03\n",
      "Num timesteps: 219000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1101.59\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1088.90\n",
      "Num timesteps: 221000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1089.38\n",
      "Num timesteps: 222000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1082.66\n",
      "Num timesteps: 223000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1080.98\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1077.05\n",
      "Num timesteps: 225000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1068.45\n",
      "Num timesteps: 226000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1065.74\n",
      "Num timesteps: 227000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1046.08\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1044.77\n",
      "Num timesteps: 229000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1045.14\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1043.97\n",
      "Num timesteps: 231000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1042.89\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1023.70\n",
      "Num timesteps: 233000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1016.43\n",
      "Num timesteps: 234000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1007.41\n",
      "Num timesteps: 235000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1007.50\n",
      "Num timesteps: 236000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1000.57\n",
      "Num timesteps: 237000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 1000.19\n",
      "Num timesteps: 238000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 980.27\n",
      "Num timesteps: 239000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 978.65\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 975.63\n",
      "Num timesteps: 241000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 974.58\n",
      "Num timesteps: 242000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 968.72\n",
      "Num timesteps: 243000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 943.54\n",
      "Num timesteps: 244000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 889.14\n",
      "Num timesteps: 245000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 888.64\n",
      "Num timesteps: 246000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 878.23\n",
      "Num timesteps: 247000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 800.24\n",
      "Num timesteps: 248000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 792.52\n",
      "Num timesteps: 249000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 764.36\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 750.76\n",
      "Num timesteps: 251000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 729.52\n",
      "Num timesteps: 252000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 669.79\n",
      "Num timesteps: 253000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 672.17\n",
      "Num timesteps: 254000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 677.12\n",
      "Num timesteps: 255000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 677.09\n",
      "Num timesteps: 256000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 682.04\n",
      "Num timesteps: 257000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 681.28\n",
      "Num timesteps: 258000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 684.50\n",
      "Num timesteps: 259000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 688.84\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 693.93\n",
      "Num timesteps: 261000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 695.85\n",
      "Num timesteps: 262000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 696.69\n",
      "Num timesteps: 263000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 701.89\n",
      "Num timesteps: 264000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 707.35\n",
      "Num timesteps: 265000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 708.93\n",
      "Num timesteps: 266000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 711.33\n",
      "Num timesteps: 267000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 714.49\n",
      "Num timesteps: 268000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 719.36\n",
      "Num timesteps: 269000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 719.28\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 721.92\n",
      "Num timesteps: 271000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 724.74\n",
      "Num timesteps: 272000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 730.34\n",
      "Num timesteps: 273000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 734.97\n",
      "Num timesteps: 274000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 738.40\n",
      "Num timesteps: 275000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 739.24\n",
      "Num timesteps: 276000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 741.46\n",
      "Num timesteps: 277000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 743.50\n",
      "Num timesteps: 278000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 746.65\n",
      "Num timesteps: 279000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 747.72\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 750.26\n",
      "Num timesteps: 281000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 753.17\n",
      "Num timesteps: 282000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 756.25\n",
      "Num timesteps: 283000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 758.26\n",
      "Num timesteps: 284000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 764.63\n",
      "Num timesteps: 285000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 767.16\n",
      "Num timesteps: 286000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 770.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 287000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 770.36\n",
      "Num timesteps: 288000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 770.36\n",
      "Num timesteps: 289000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 772.88\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 776.42\n",
      "Num timesteps: 291000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.30\n",
      "Num timesteps: 292000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 778.69\n",
      "Num timesteps: 293000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.74\n",
      "Num timesteps: 294000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 784.06\n",
      "Num timesteps: 295000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 788.05\n",
      "Num timesteps: 296000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 789.10\n",
      "Num timesteps: 297000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 791.64\n",
      "Num timesteps: 298000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 790.47\n",
      "Num timesteps: 299000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 794.21\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 794.02\n",
      "Num timesteps: 301000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 792.09\n",
      "Num timesteps: 302000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 792.82\n",
      "Num timesteps: 303000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 793.76\n",
      "Num timesteps: 304000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 795.15\n",
      "Num timesteps: 305000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 794.38\n",
      "Num timesteps: 306000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 794.90\n",
      "Num timesteps: 307000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 797.90\n",
      "Num timesteps: 308000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 798.33\n",
      "Num timesteps: 309000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 796.62\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 787.31\n",
      "Num timesteps: 311000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 787.17\n",
      "Num timesteps: 312000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 789.88\n",
      "Num timesteps: 313000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 785.35\n",
      "Num timesteps: 314000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 785.35\n",
      "Num timesteps: 315000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 782.60\n",
      "Num timesteps: 316000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 782.13\n",
      "Num timesteps: 317000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 779.35\n",
      "Num timesteps: 318000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.91\n",
      "Num timesteps: 319000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.01\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 775.85\n",
      "Num timesteps: 321000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 775.29\n",
      "Num timesteps: 322000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 773.58\n",
      "Num timesteps: 323000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 772.66\n",
      "Num timesteps: 324000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 770.78\n",
      "Num timesteps: 325000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 771.49\n",
      "Num timesteps: 326000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 772.51\n",
      "Num timesteps: 327000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 774.88\n",
      "Num timesteps: 328000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 776.80\n",
      "Num timesteps: 329000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.50\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 778.58\n",
      "Num timesteps: 331000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 775.02\n",
      "Num timesteps: 332000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 778.37\n",
      "Num timesteps: 333000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.26\n",
      "Num timesteps: 334000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 774.14\n",
      "Num timesteps: 335000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 776.76\n",
      "Num timesteps: 336000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 777.07\n",
      "Num timesteps: 337000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 780.95\n",
      "Num timesteps: 338000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 785.08\n",
      "Num timesteps: 339000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 789.72\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 789.82\n",
      "Num timesteps: 341000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 789.25\n",
      "Num timesteps: 342000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 789.05\n",
      "Num timesteps: 343000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 787.87\n",
      "Num timesteps: 344000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 788.09\n",
      "Num timesteps: 345000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 793.04\n",
      "Num timesteps: 346000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 791.90\n",
      "Num timesteps: 347000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 794.54\n",
      "Num timesteps: 348000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 799.84\n",
      "Num timesteps: 349000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 806.30\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 807.39\n",
      "Num timesteps: 351000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 805.85\n",
      "Num timesteps: 352000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 805.69\n",
      "Num timesteps: 353000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 809.63\n",
      "Num timesteps: 354000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 808.16\n",
      "Num timesteps: 355000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 807.97\n",
      "Num timesteps: 356000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 817.49\n",
      "Num timesteps: 357000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 816.33\n",
      "Num timesteps: 358000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 822.00\n",
      "Num timesteps: 359000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 822.49\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 825.88\n",
      "Num timesteps: 361000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 826.34\n",
      "Num timesteps: 362000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 830.83\n",
      "Num timesteps: 363000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 833.33\n",
      "Num timesteps: 364000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 833.33\n",
      "Num timesteps: 365000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 836.94\n",
      "Num timesteps: 366000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 837.71\n",
      "Num timesteps: 367000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 837.56\n",
      "Num timesteps: 368000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 835.28\n",
      "Num timesteps: 369000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 835.02\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 835.30\n",
      "Num timesteps: 371000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 837.25\n",
      "Num timesteps: 372000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 841.88\n",
      "Num timesteps: 373000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 843.42\n",
      "Num timesteps: 374000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 843.42\n",
      "Num timesteps: 375000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 851.42\n",
      "Num timesteps: 376000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 850.70\n",
      "Num timesteps: 377000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 846.82\n",
      "Num timesteps: 378000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 847.47\n",
      "Num timesteps: 379000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 847.16\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 849.14\n",
      "Num timesteps: 381000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 846.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 382000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 849.16\n",
      "Num timesteps: 383000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 851.59\n",
      "Num timesteps: 384000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 849.06\n",
      "Num timesteps: 385000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 849.06\n",
      "Num timesteps: 386000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 853.44\n",
      "Num timesteps: 387000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 855.77\n",
      "Num timesteps: 388000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 862.05\n",
      "Num timesteps: 389000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 860.81\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 859.22\n",
      "Num timesteps: 391000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 858.66\n",
      "Num timesteps: 392000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 856.89\n",
      "Num timesteps: 393000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 858.13\n",
      "Num timesteps: 394000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 859.74\n",
      "Num timesteps: 395000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 862.70\n",
      "Num timesteps: 396000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 860.89\n",
      "Num timesteps: 397000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 860.89\n",
      "Num timesteps: 398000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 867.79\n",
      "Num timesteps: 399000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 872.18\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 879.85\n",
      "Num timesteps: 401000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 886.30\n",
      "Num timesteps: 402000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 886.30\n",
      "Num timesteps: 403000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 894.52\n",
      "Num timesteps: 404000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 894.52\n",
      "Num timesteps: 405000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 904.65\n",
      "Num timesteps: 406000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 906.85\n",
      "Num timesteps: 407000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 910.01\n",
      "Num timesteps: 408000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 914.28\n",
      "Num timesteps: 409000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 920.81\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 921.41\n",
      "Num timesteps: 411000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 920.41\n",
      "Num timesteps: 412000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 923.12\n",
      "Num timesteps: 413000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 926.62\n",
      "Num timesteps: 414000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 928.96\n",
      "Num timesteps: 415000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 938.13\n",
      "Num timesteps: 416000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 938.13\n",
      "Num timesteps: 417000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 942.62\n",
      "Num timesteps: 418000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 939.69\n",
      "Num timesteps: 419000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 938.37\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 933.30\n",
      "Num timesteps: 421000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 933.32\n",
      "Num timesteps: 422000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 930.35\n",
      "Num timesteps: 423000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 928.74\n",
      "Num timesteps: 424000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 926.36\n",
      "Num timesteps: 425000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 919.43\n",
      "Num timesteps: 426000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 916.40\n",
      "Num timesteps: 427000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 907.49\n",
      "Num timesteps: 428000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 904.49\n",
      "Num timesteps: 429000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 897.89\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 894.29\n",
      "Num timesteps: 431000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 894.59\n",
      "Num timesteps: 432000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 892.34\n",
      "Num timesteps: 433000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 885.67\n",
      "Num timesteps: 434000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 885.04\n",
      "Num timesteps: 435000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 882.33\n",
      "Num timesteps: 436000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 871.71\n",
      "Num timesteps: 437000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 868.48\n",
      "Num timesteps: 438000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 868.76\n",
      "Num timesteps: 439000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 869.94\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 867.16\n",
      "Num timesteps: 441000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 869.41\n",
      "Num timesteps: 442000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 858.20\n",
      "Num timesteps: 443000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 857.65\n",
      "Num timesteps: 444000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 847.95\n",
      "Num timesteps: 445000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 844.68\n",
      "Num timesteps: 446000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 839.83\n",
      "Num timesteps: 447000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 836.42\n",
      "Num timesteps: 448000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 831.18\n",
      "Num timesteps: 449000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 830.99\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 829.97\n",
      "Num timesteps: 451000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 827.14\n",
      "Num timesteps: 452000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 826.15\n",
      "Num timesteps: 453000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 819.52\n",
      "Num timesteps: 454000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 811.13\n",
      "Num timesteps: 455000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 803.57\n",
      "Num timesteps: 456000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 800.68\n",
      "Num timesteps: 457000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 802.54\n",
      "Num timesteps: 458000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 801.86\n",
      "Num timesteps: 459000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 802.31\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 801.40\n",
      "Num timesteps: 461000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 796.47\n",
      "Num timesteps: 462000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 792.76\n",
      "Num timesteps: 463000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 790.41\n",
      "Num timesteps: 464000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 787.13\n",
      "Num timesteps: 465000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 775.42\n",
      "Num timesteps: 466000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 773.82\n",
      "Num timesteps: 467000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 772.74\n",
      "Num timesteps: 468000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 773.72\n",
      "Num timesteps: 469000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 773.03\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 770.65\n",
      "Num timesteps: 471000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 766.41\n",
      "Num timesteps: 472000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 760.89\n",
      "Num timesteps: 473000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 755.86\n",
      "Num timesteps: 474000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 752.78\n",
      "Num timesteps: 475000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 749.28\n",
      "Num timesteps: 476000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 752.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 477000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 740.95\n",
      "Num timesteps: 478000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 748.01\n",
      "Num timesteps: 479000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 739.77\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 739.08\n",
      "Num timesteps: 481000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 736.41\n",
      "Num timesteps: 482000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 739.14\n",
      "Num timesteps: 483000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 739.41\n",
      "Num timesteps: 484000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 739.03\n",
      "Num timesteps: 485000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 740.31\n",
      "Num timesteps: 486000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 736.86\n",
      "Num timesteps: 487000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 735.71\n",
      "Num timesteps: 488000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 729.11\n",
      "Num timesteps: 489000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 726.92\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 726.57\n",
      "Num timesteps: 491000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 727.73\n",
      "Num timesteps: 492000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 732.68\n",
      "Num timesteps: 493000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 733.38\n",
      "Num timesteps: 494000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 736.77\n",
      "Num timesteps: 495000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 740.96\n",
      "Num timesteps: 496000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 743.34\n",
      "Num timesteps: 497000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 743.77\n",
      "Num timesteps: 498000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 745.89\n",
      "Num timesteps: 499000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 745.88\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 752.76\n",
      "Num timesteps: 501000\n",
      "Best mean reward: 2036.00 - Last mean reward per episode: 756.75\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model = PPO(\"MlpPolicy\", env).learn(total_timesteps=ts, callback=callback)\n",
    "end=time.time()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 13.94647827943166 min\n"
     ]
    }
   ],
   "source": [
    "print('Total time taken: {} min'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.A=0.3\n",
    "env.E=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "THR=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<100:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    if obs[0]<THR and obs[0]>0.8*THR: # use when minimizing displacement\n",
    "        break\n",
    "#     if obs[0]<THR or obs[0]>3*THR : # use when minimizing cross-sectional area\n",
    "#         break  \n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged! MaxDispl=1.9062606657432373, A=2.021107671573183,E=0.34607242762415025\n"
     ]
    }
   ],
   "source": [
    "if obs[0]>THR:\n",
    "    print(\"Bad initial parameters! Try increasing initial cross-sectional area A, Young's modulus E and/or number of iterations\")\n",
    "elif obs[0]<0.8*THR:\n",
    "    print(\"You can get better parameters. Try decreasing initial area A and/or Young's modulus E\")\n",
    "else:    \n",
    "    print(\"Solution converged! MaxDispl={}, A={},E={}\".format(obs[0],obs[1],obs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], ts, results_plotter.X_TIMESTEPS, \"PPO BarEnv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
