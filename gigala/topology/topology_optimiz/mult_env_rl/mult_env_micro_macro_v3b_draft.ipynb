{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfd9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np                                                \n",
    "import matplotlib.pyplot as plt                                   \n",
    "import autograd, autograd.core, autograd.extend, autograd.tracer  \n",
    "import autograd.numpy as anp      \n",
    "import scipy, scipy.ndimage, scipy.sparse, scipy.sparse.linalg    \n",
    "                                                     \n",
    "import gym\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import results_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52efed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abda2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13e98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78e7e5",
   "metadata": {},
   "source": [
    "Aim of the moddeling:\n",
    "\n",
    "- if you want to train multiple envs at once e.g. to do multiple abstraction levels design between: comprehensive levels as per Bathe, or scale (micro, macro), or different optimization objectives, or different action space, you can use SB3 Vectorized Environments in the below manner. For instance, for two or more Env you can combine the reward in any specific way thus ensuring communication between the abstraction levels.\n",
    "- This notebook aims at imitating simultaneous micro and macro topology optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e76bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0, x0 = 5, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450803d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "    \n",
    "def get_args(normals, forces, density=1e-4):  # Manage the problem setup parameters\n",
    "    width = normals.shape[0] - 1\n",
    "    height = normals.shape[1] - 1\n",
    "    fixdofs = np.flatnonzero(normals.ravel())\n",
    "    alldofs = np.arange(2 * (width + 1) * (height + 1))\n",
    "    freedofs = np.sort(list(set(alldofs) - set(fixdofs)))\n",
    "    params = {\n",
    "      # material properties\n",
    "      'young': 1, 'young_min': 1e-9, 'poisson': 0.3, 'g': 0,\n",
    "      # constraints\n",
    "      'density': density, 'xmin': 0.001, 'xmax': 1.0,\n",
    "      # input parameters\n",
    "      'nelx': width, 'nely': height, 'mask': 1, 'penal': 3.0, 'filter_width': 1,\n",
    "      'freedofs': freedofs, 'fixdofs': fixdofs, 'forces': forces.ravel(),\n",
    "      # optimization parameters\n",
    "      'opt_steps': 80, 'print_every': 10}\n",
    "    return ObjectView(params)\n",
    "\n",
    "def mbb_beam(width=y0, height=x0, density=1e-4, y=1, x=0, rd=-1):  # textbook beam example\n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    normals[0, 0, x] = 1\n",
    "    normals[0, 0, y] = 1\n",
    "    normals[0, -1, x] = 1\n",
    "    normals[0, -1, y] = 1\n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    forces[-1, rd, y] = -1\n",
    "    return normals, forces, density\n",
    "\n",
    "def mbb_beam_micro(width=y0, height=x0, density=1e-4, y=1, x=0, rd=-1):  # textbook beam example\n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    \n",
    "    normals[2, 1, x] = 1 #!!!!!!!!!!!!!!!!should be adjusted to account for any dimensions!!!!!!!!!!!!!!!!!!\n",
    "    normals[2, 1, y] = 1 #!!!!!!!!!!!!!!!!should be adjusted to account for any dimension!!!!!!!!!!!!!!!!!!!\n",
    "    normals[2, 3, x] = 1 #!!!!!!!!!!!!!!!!should be adjusted to account for any dimension!!!!!!!!!!!!!!!!!!!\n",
    "    normals[2, 3, y] = 1 #!!!!!!!!!!!!!!!!should be adjusted to account for any dimension!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    \n",
    "    a=random.choice([1,-1])\n",
    "    b=random.choice([1,-1])\n",
    "    \n",
    "    for j in range(0, width):\n",
    "        forces[0, j, y] = a\n",
    "        forces[height, j, y] = -a \n",
    "        \n",
    "    for j in range(0, height):\n",
    "        forces[j, 0, x] = b\n",
    "        forces[j, width, x] = -b\n",
    "    \n",
    "    \n",
    "    return normals, forces, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c8a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def young_modulus(x, e_0, e_min, p=3):\n",
    "    return e_min + x ** p * (e_0 - e_min)\n",
    "\n",
    "def physical_density(x, args, volume_contraint=False, use_filter=True):\n",
    "    x = args.mask * x.reshape(args.nely, args.nelx)  # reshape from 1D to 2D\n",
    "    return gaussian_filter(x, args.filter_width) if use_filter else x  # maybe filter\n",
    "\n",
    "def mean_density(x, args, volume_contraint=False, use_filter=True):\n",
    "    return anp.mean(physical_density(x, args, volume_contraint, use_filter)) / anp.mean(args.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292f40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, args, volume_contraint=False, use_filter=True):\n",
    "    kwargs = dict(penal=args.penal, e_min=args.young_min, e_0=args.young)\n",
    "    x_phys = physical_density(x, args, volume_contraint=volume_contraint, use_filter=use_filter)\n",
    "    ke     = get_stiffness_matrix(args.young, args.poisson)  # stiffness matrix\n",
    "    u      = displace(x_phys, ke, args.forces, args.freedofs, args.fixdofs, **kwargs)\n",
    "    c      = compliance(x_phys, u, ke, **kwargs)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dea59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@autograd.extend.primitive\n",
    "def gaussian_filter(x, width): # 2D gaussian blur/filter\n",
    "    return scipy.ndimage.gaussian_filter(x, width, mode='reflect')\n",
    "\n",
    "def _gaussian_filter_vjp(ans, x, width): # gives the gradient of orig. function w.r.t. x\n",
    "    del ans, x  # unused\n",
    "    return lambda g: gaussian_filter(g, width)\n",
    "autograd.extend.defvjp(gaussian_filter, _gaussian_filter_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c04ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compliance(x_phys, u, ke, *, penal=3, e_min=1e-9, e_0=1):\n",
    "    nely, nelx = x_phys.shape\n",
    "    ely, elx = anp.meshgrid(range(nely), range(nelx))  # x, y coords for the index map\n",
    "\n",
    "    n1 = (nely+1)*(elx+0) + (ely+0)  # nodes\n",
    "    n2 = (nely+1)*(elx+1) + (ely+0)\n",
    "    n3 = (nely+1)*(elx+1) + (ely+1)\n",
    "    n4 = (nely+1)*(elx+0) + (ely+1)\n",
    "    all_ixs = anp.array([2*n1, 2*n1+1, 2*n2, 2*n2+1, 2*n3, 2*n3+1, 2*n4, 2*n4+1])\n",
    "    u_selected = u[all_ixs]  # select from u matrix\n",
    "\n",
    "    ke_u = anp.einsum('ij,jkl->ikl', ke, u_selected)  # compute x^penal * U.T @ ke @ U\n",
    "    ce = anp.einsum('ijk,ijk->jk', u_selected, ke_u)\n",
    "    C = young_modulus(x_phys, e_0, e_min, p=penal) * ce.T\n",
    "    return anp.sum(C)\n",
    "\n",
    "def get_stiffness_matrix(e, nu):  # e=young's modulus, nu=poisson coefficient\n",
    "    k = anp.array([1/2-nu/6, 1/8+nu/8, -1/4-nu/12, -1/8+3*nu/8,\n",
    "                -1/4+nu/12, -1/8-nu/8, nu/6, 1/8-3*nu/8])\n",
    "    return e/(1-nu**2)*anp.array([[k[0], k[1], k[2], k[3], k[4], k[5], k[6], k[7]],\n",
    "                               [k[1], k[0], k[7], k[6], k[5], k[4], k[3], k[2]],\n",
    "                               [k[2], k[7], k[0], k[5], k[6], k[3], k[4], k[1]],\n",
    "                               [k[3], k[6], k[5], k[0], k[7], k[2], k[1], k[4]],\n",
    "                               [k[4], k[5], k[6], k[7], k[0], k[1], k[2], k[3]],\n",
    "                               [k[5], k[4], k[3], k[2], k[1], k[0], k[7], k[6]],\n",
    "                               [k[6], k[3], k[4], k[1], k[2], k[7], k[0], k[5]],\n",
    "                               [k[7], k[2], k[1], k[4], k[3], k[6], k[5], k[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "278ac963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k(stiffness, ke):\n",
    "    # Constructs sparse stiffness matrix k (used in the displace fn)\n",
    "    # First, get position of the nodes of each element in the stiffness matrix\n",
    "    nely, nelx = stiffness.shape\n",
    "    ely, elx = anp.meshgrid(range(nely), range(nelx))  # x, y coords\n",
    "    ely, elx = ely.reshape(-1, 1), elx.reshape(-1, 1)\n",
    "\n",
    "    n1 = (nely+1)*(elx+0) + (ely+0)\n",
    "    n2 = (nely+1)*(elx+1) + (ely+0)\n",
    "    n3 = (nely+1)*(elx+1) + (ely+1)\n",
    "    n4 = (nely+1)*(elx+0) + (ely+1)\n",
    "    edof = anp.array([2*n1, 2*n1+1, 2*n2, 2*n2+1, 2*n3, 2*n3+1, 2*n4, 2*n4+1])\n",
    "    edof = edof.T[0]\n",
    "    x_list = anp.repeat(edof, 8)  # flat list pointer of each node in an element\n",
    "    y_list = anp.tile(edof, 8).flatten()  # flat list pointer of each node in elem\n",
    "\n",
    "    # make the global stiffness matrix K\n",
    "    kd = stiffness.T.reshape(nelx*nely, 1, 1)\n",
    "    value_list = (kd * anp.tile(ke, kd.shape)).flatten()\n",
    "    return value_list, y_list, x_list\n",
    "\n",
    "def displace(x_phys, ke, forces, freedofs, fixdofs, *, penal=3, e_min=1e-9, e_0=1):\n",
    "    # Displaces the load x using finite element techniques (solve_coo=most of runtime)\n",
    "    stiffness = young_modulus(x_phys, e_0, e_min, p=penal)\n",
    "    k_entries, k_ylist, k_xlist = get_k(stiffness, ke)\n",
    "\n",
    "    index_map, keep, indices = _get_dof_indices(freedofs, fixdofs, k_ylist, k_xlist)\n",
    "\n",
    "    u_nonzero = solve_coo(k_entries[keep], indices, forces[freedofs], sym_pos=True)\n",
    "    u_values = anp.concatenate([u_nonzero, anp.zeros(len(fixdofs))])\n",
    "    return u_values[index_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "911a43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dof_indices(freedofs, fixdofs, k_xlist, k_ylist):\n",
    "    index_map = inverse_permutation(anp.concatenate([freedofs, fixdofs]))\n",
    "    keep = anp.isin(k_xlist, freedofs) & anp.isin(k_ylist, freedofs)\n",
    "    # Now we index an indexing array that is being indexed by the indices of k\n",
    "    i = index_map[k_ylist][keep]\n",
    "    j = index_map[k_xlist][keep]\n",
    "    return index_map, keep, anp.stack([i, j])\n",
    "\n",
    "def inverse_permutation(indices):  # reverses an index operation\n",
    "    inverse_perm = np.zeros(len(indices), dtype=anp.int64)\n",
    "    inverse_perm[indices] = np.arange(len(indices), dtype=anp.int64)\n",
    "    return inverse_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f85161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_solver(a_entries, a_indices, size, sym_pos):\n",
    "    # a is (usu.) symmetric positive; could solve 2x faster w/sksparse.cholmod.cholesky(a).solve_A\n",
    "    a = scipy.sparse.coo_matrix((a_entries, a_indices), shape=(size,)*2).tocsc()\n",
    "    return scipy.sparse.linalg.splu(a).solve\n",
    "\n",
    "@autograd.primitive\n",
    "def solve_coo(a_entries, a_indices, b, sym_pos=False):\n",
    "    solver = _get_solver(a_entries, a_indices, b.size, sym_pos)\n",
    "    return solver(b)\n",
    "\n",
    "def grad_solve_coo_entries(ans, a_entries, a_indices, b, sym_pos=False):\n",
    "    def jvp(grad_ans):\n",
    "        lambda_ = solve_coo(a_entries, a_indices if sym_pos else a_indices[::-1],\n",
    "                            grad_ans, sym_pos)\n",
    "        i, j = a_indices\n",
    "        return -lambda_[i] * ans[j]\n",
    "    return jvp\n",
    "\n",
    "autograd.extend.defvjp(solve_coo, grad_solve_coo_entries,\n",
    "                       lambda: print('err: gradient undefined'),\n",
    "                       lambda: print('err: gradient not implemented'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daa2068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, x):\n",
    "        self.flag_ = True\n",
    "#         self.flag_ = False\n",
    "        self.n, self.m = x.shape\n",
    "        self.actions_dic={} \n",
    "    \n",
    "        k=0\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.m):\n",
    "                self.actions_dic[k]=(i,j)\n",
    "                k+=1\n",
    "        \n",
    "    def action_space_(self, action, X):\n",
    "        x,y=self.actions_dic[action]\n",
    "        X[x][y]=1\n",
    "        \n",
    "    def draw(self,X):  \n",
    "        plt.figure(dpi=50) \n",
    "        print('\\nFinal Cantilever beam design:')\n",
    "        plt.imshow(X) \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be6b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_stopt(args, x):\n",
    "\n",
    "    reshape = lambda x: x.reshape(args.nely, args.nelx)\n",
    "    objective_fn = lambda x: objective(reshape(x), args)\n",
    "#     constraint = lambda params: mean_density(reshape(params), args) - args.density\n",
    "    constraint = lambda params: mean_density(reshape(params), args) \n",
    "    value = objective_fn(x)\n",
    "    const = constraint(x)\n",
    "    return value, const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0870a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CantileverEnv(gymnasium.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.rd=0\n",
    "        self.args = get_args(*mbb_beam(rd=self.rd))\n",
    "        \n",
    "        DIM=self.args.nelx*self.args.nely\n",
    "        N_DISCRETE_ACTIONS=self.args.nelx*self.args.nely\n",
    "        \n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.observation_space = spaces.Box(low=np.array([-1e10 for x in range(DIM)]),\n",
    "                                            high=np.array([1e10 for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.float64)\n",
    "        \n",
    " \n",
    "        self.x = anp.ones((self.args.nely, self.args.nelx))*self.args.density \n",
    "    \n",
    "        self.M=Model(self.x)\n",
    "        \n",
    "        self.reward=0\n",
    "        self.step_=0\n",
    "        self.needs_reset = True\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.args = get_args(*mbb_beam(rd=self.rd))\n",
    "        \n",
    "        self.M.action_space_(action, self.x)\n",
    "        \n",
    "        tmp, const = fast_stopt(self.args, self.x)\n",
    "        \n",
    "        self.step_+=1\n",
    "        \n",
    "        a1=1/tmp\n",
    "        a2=1-const\n",
    "        self.reward =2*a1*a2/(a1+a2)\n",
    "       \n",
    "        done=False\n",
    "                  \n",
    "        if self.step_>self.M.n*self.M.m:\n",
    "            done=True\n",
    "            \n",
    "        if const>0.7:\n",
    "            done=True\n",
    "            \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "                         \n",
    "      \n",
    "        return self.x.reshape(self.x.shape[0]*self.x.shape[1]), self.reward, done,False, dict()\n",
    "\n",
    "    def reset(self,seed=0):\n",
    "        \n",
    "        if not self.M.flag_:\n",
    "            self.rd=random.choice([0,2,-2])\n",
    "        else:\n",
    "            self.rd=-1\n",
    "           \n",
    "        self.x = anp.ones((self.args.nely, self.args.nelx))*self.args.density \n",
    "\n",
    "        self.reward=0\n",
    "        self.needs_reset = False\n",
    "        self.step_=0\n",
    "\n",
    "        return self.x.reshape(self.x.shape[0]*self.x.shape[1]),{}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.M.draw(self.x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eb2c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CantileverEnvMicro(gymnasium.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.rd=0\n",
    "        self.args1 = get_args(*mbb_beam(rd=self.rd)) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        DIM=self.args1.nelx*self.args1.nely\n",
    "        N_DISCRETE_ACTIONS=self.args1.nelx*self.args1.nely\n",
    "        \n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.observation_space = spaces.Box(low=np.array([-1e10 for x in range(DIM)]),\n",
    "                                            high=np.array([1e10 for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.float64)\n",
    "        \n",
    " \n",
    "        self.x = anp.ones((self.args1.nely, self.args1.nelx))*self.args1.density \n",
    "    \n",
    "        self.M=Model(self.x)\n",
    "        \n",
    "        self.reward=0\n",
    "        self.step_=0\n",
    "        self.needs_reset = True\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.args1 = get_args(*mbb_beam(rd=self.rd)) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        self.M.action_space_(action, self.x)\n",
    "        \n",
    "        tmp, const = fast_stopt(self.args1, self.x)\n",
    "        \n",
    "        self.step_+=1\n",
    "        \n",
    "        \n",
    "        a1=1/tmp\n",
    "        a2=1-const\n",
    "        self.reward =2*a1*a2/(a1+a2)\n",
    "       \n",
    "        done=False\n",
    "                  \n",
    "        if self.step_>self.M.n*self.M.m:\n",
    "            done=True\n",
    "            \n",
    "        if const>0.7:\n",
    "            done=True\n",
    "            \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "                         \n",
    "      \n",
    "        return self.x.reshape(self.x.shape[0]*self.x.shape[1]), self.reward, done,False, dict()\n",
    "\n",
    "    def reset(self,seed=0):\n",
    "        \n",
    "        if not self.M.flag_:\n",
    "            self.rd=random.choice([0,2,-2])\n",
    "        else:\n",
    "            self.rd=0     #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "           \n",
    "        self.x = anp.ones((self.args1.nely, self.args1.nelx))*self.args1.density \n",
    "\n",
    "        self.reward=0\n",
    "        self.needs_reset = False\n",
    "        self.step_=0\n",
    "\n",
    "        return self.x.reshape(self.x.shape[0]*self.x.shape[1]),{}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.M.draw(self.x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21474758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "#             print(y)\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                \n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                \n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0493fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossRewardEnv(gymnasium.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.env1 = CantileverEnv()\n",
    "        self.env2 = CantileverEnvMicro()\n",
    "\n",
    "        # Combine observation spaces (you could process these differently)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'macro': self.env1.observation_space,\n",
    "            'micro': self.env2.observation_space\n",
    "        })\n",
    "\n",
    "        # Combine action spaces into a Dict space\n",
    "        self.action_space = spaces.MultiDiscrete([25, 25])\n",
    "\n",
    "    def reset(self,seed=0):\n",
    "        obs1, info1 = self.env1.reset()\n",
    "        obs2, info2 = self.env2.reset()\n",
    "        return {\n",
    "            'macro': obs1,\n",
    "            'micro': obs2\n",
    "        }, {\n",
    "            'macro': info1,\n",
    "            'micro': info2\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        a1 = action[0]\n",
    "        a2 = action[1]\n",
    "\n",
    "        obs1, r1, done1,_, info1 = self.env1.step(a1)\n",
    "        obs2, r2, done2,_, info2 = self.env2.step(a2)\n",
    "\n",
    "        cross_reward1 = 2*r1*r2/(r1+r2)\n",
    "        cross_reward2 = r2 \n",
    "\n",
    "        done = done1 or done2  # or you could choose more complex logic\n",
    "\n",
    "        return {\n",
    "            'macro': obs1,\n",
    "            'micro': obs2\n",
    "        }, (cross_reward1+cross_reward2), done, False, {\n",
    "            'macro': info1,\n",
    "            'micro': info2\n",
    "        }\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.env1.render()\n",
    "        self.env2.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env1.close()\n",
    "        self.env2.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5b62f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(X):  \n",
    "    plt.figure(dpi=50) \n",
    "    print('\\nFinal Cantilever beam design:')\n",
    "    plt.imshow(X.reshape(y0, x0)) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3998730",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1c8d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym6_2o1o1/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce7820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a95daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80cdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f55a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a44f772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=CrossRewardEnv()\n",
    "check_env(env)\n",
    "env  = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5a23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebfbc50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: 0.37\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 0.37 - Last mean reward per episode: 0.35\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 0.37 - Last mean reward per episode: 0.37\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 0.37 - Last mean reward per episode: 0.36\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 0.37 - Last mean reward per episode: 0.36\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 0.37 - Last mean reward per episode: 0.35\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 0.37 - Last mean reward per episode: 0.39\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 0.39 - Last mean reward per episode: 0.44\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 0.44 - Last mean reward per episode: 0.45\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 0.45 - Last mean reward per episode: 0.46\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 0.46 - Last mean reward per episode: 0.46\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 0.46 - Last mean reward per episode: 0.47\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 0.47 - Last mean reward per episode: 0.49\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 0.49 - Last mean reward per episode: 0.48\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 0.49 - Last mean reward per episode: 0.51\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 0.51 - Last mean reward per episode: 0.54\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 0.54 - Last mean reward per episode: 0.59\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 0.59 - Last mean reward per episode: 0.61\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 0.61 - Last mean reward per episode: 0.60\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 0.61 - Last mean reward per episode: 0.59\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 0.61 - Last mean reward per episode: 0.59\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 0.61 - Last mean reward per episode: 0.62\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 0.62 - Last mean reward per episode: 0.66\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 0.66 - Last mean reward per episode: 0.67\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 0.67 - Last mean reward per episode: 0.70\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 0.70 - Last mean reward per episode: 0.68\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 0.70 - Last mean reward per episode: 0.70\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 0.70 - Last mean reward per episode: 0.71\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 0.71 - Last mean reward per episode: 0.72\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 0.72 - Last mean reward per episode: 0.74\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 0.74 - Last mean reward per episode: 0.75\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 0.75 - Last mean reward per episode: 0.76\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 0.76 - Last mean reward per episode: 0.78\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 0.78 - Last mean reward per episode: 0.79\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 0.79 - Last mean reward per episode: 0.78\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 0.79 - Last mean reward per episode: 0.79\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 0.79 - Last mean reward per episode: 0.80\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 0.80 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 0.80 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 0.80 - Last mean reward per episode: 0.83\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 0.83 - Last mean reward per episode: 0.83\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 0.83 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 0.83 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 0.83 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 0.83 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 0.83 - Last mean reward per episode: 0.84\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 0.84 - Last mean reward per episode: 0.85\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.85\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.86\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 0.86 - Last mean reward per episode: 0.87\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 0.87 - Last mean reward per episode: 0.88\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 0.88 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 0.88 - Last mean reward per episode: 0.90\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 0.90 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 0.90 - Last mean reward per episode: 0.92\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.93\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 0.93 - Last mean reward per episode: 0.93\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 0.93 - Last mean reward per episode: 0.94\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.98\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.98\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.99\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 1.02\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 1.02 - Last mean reward per episode: 1.05\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 73000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 0.99\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 0.98\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.01\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.04\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.02\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.04\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.02\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.03\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.05\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 1.05 - Last mean reward per episode: 1.06\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 1.06 - Last mean reward per episode: 1.06\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 1.06 - Last mean reward per episode: 1.07\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 1.07 - Last mean reward per episode: 1.06\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 1.07 - Last mean reward per episode: 1.05\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 1.07 - Last mean reward per episode: 1.06\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 1.07 - Last mean reward per episode: 1.10\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 1.10 - Last mean reward per episode: 1.09\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 1.10 - Last mean reward per episode: 1.08\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 1.10 - Last mean reward per episode: 1.04\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 1.10 - Last mean reward per episode: 1.04\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 1.10 - Last mean reward per episode: 1.08\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 1.10 - Last mean reward per episode: 1.13\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 1.13 - Last mean reward per episode: 1.15\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.13\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.12\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.13\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.15\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.15\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.13\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.14\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 1.15 - Last mean reward per episode: 1.16\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 1.16 - Last mean reward per episode: 1.13\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 1.16 - Last mean reward per episode: 1.13\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 1.16 - Last mean reward per episode: 1.13\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 1.16 - Last mean reward per episode: 1.15\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 1.16 - Last mean reward per episode: 1.18\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 1.18 - Last mean reward per episode: 1.19\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 1.21\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.19\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.19\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.18\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.20\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.19\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 1.21 - Last mean reward per episode: 1.22\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.22\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.19\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.18\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.17\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.18\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.18\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.22\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.22\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 1.22 - Last mean reward per episode: 1.23\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 1.23 - Last mean reward per episode: 1.24\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.23\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.22\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.23\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.23\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.24\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.24\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.25\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 1.25 - Last mean reward per episode: 1.25\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 1.25 - Last mean reward per episode: 1.27\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 1.27 - Last mean reward per episode: 1.27\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 1.27 - Last mean reward per episode: 1.25\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 1.27 - Last mean reward per episode: 1.21\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 1.27 - Last mean reward per episode: 1.23\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 1.27 - Last mean reward per episode: 1.30\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 1.30 - Last mean reward per episode: 1.30\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 1.30 - Last mean reward per episode: 1.28\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 1.30 - Last mean reward per episode: 1.28\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 1.30 - Last mean reward per episode: 1.29\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 1.30 - Last mean reward per episode: 1.31\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.31\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.32\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.30\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 157000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.30\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.32\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.32\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.32\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.29\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.29\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.31\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.32\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.30\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.32\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 1.32 - Last mean reward per episode: 1.33\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 1.33 - Last mean reward per episode: 1.33\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 1.33 - Last mean reward per episode: 1.34\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.35\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.35\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.34\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.35\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.32\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.31\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.33\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 1.35 - Last mean reward per episode: 1.36\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 1.36 - Last mean reward per episode: 1.36\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 1.36 - Last mean reward per episode: 1.34\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 1.36 - Last mean reward per episode: 1.35\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 1.36 - Last mean reward per episode: 1.36\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 1.36 - Last mean reward per episode: 1.38\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 1.38 - Last mean reward per episode: 1.37\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 1.38 - Last mean reward per episode: 1.37\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 1.38 - Last mean reward per episode: 1.36\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 1.38 - Last mean reward per episode: 1.38\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 1.38 - Last mean reward per episode: 1.40\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.40\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.40\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.40\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.39\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.38\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.38\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.39\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.38\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.38\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.38\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.39\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 1.40 - Last mean reward per episode: 1.41\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.41\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.40\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.38\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.40\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.41\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.39\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 1.41 - Last mean reward per episode: 1.42\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 1.42 - Last mean reward per episode: 1.45\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.45\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.45\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.43\n",
      "Num timesteps: 211000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.43\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.44\n",
      "Num timesteps: 213000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.43\n",
      "Num timesteps: 214000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.45\n",
      "Num timesteps: 215000\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 1.46\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 1.46 - Last mean reward per episode: 1.45\n",
      "Num timesteps: 217000\n",
      "Best mean reward: 1.46 - Last mean reward per episode: 1.44\n",
      "Num timesteps: 218000\n",
      "Best mean reward: 1.46 - Last mean reward per episode: 1.44\n",
      "Num timesteps: 219000\n",
      "Best mean reward: 1.46 - Last mean reward per episode: 1.45\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 1.46 - Last mean reward per episode: 1.46\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 221000\n",
      "Best mean reward: 1.46 - Last mean reward per episode: 1.49\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 222000\n",
      "Best mean reward: 1.49 - Last mean reward per episode: 1.51\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 223000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.49\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.49\n",
      "Num timesteps: 225000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.49\n",
      "Num timesteps: 226000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.50\n",
      "Num timesteps: 227000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.51\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.50\n",
      "Num timesteps: 229000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.48\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.48\n",
      "Num timesteps: 231000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.49\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.49\n",
      "Num timesteps: 233000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.51\n",
      "Num timesteps: 234000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.51\n",
      "Num timesteps: 235000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.51\n",
      "Num timesteps: 236000\n",
      "Best mean reward: 1.51 - Last mean reward per episode: 1.52\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 237000\n",
      "Best mean reward: 1.52 - Last mean reward per episode: 1.54\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 238000\n",
      "Best mean reward: 1.54 - Last mean reward per episode: 1.56\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 239000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 240000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.50\n",
      "Num timesteps: 241000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.51\n",
      "Num timesteps: 242000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.52\n",
      "Num timesteps: 243000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.54\n",
      "Num timesteps: 244000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.54\n",
      "Num timesteps: 245000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.54\n",
      "Num timesteps: 246000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.54\n",
      "Num timesteps: 247000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.55\n",
      "Num timesteps: 248000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.51\n",
      "Num timesteps: 249000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.53\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.52\n",
      "Num timesteps: 251000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.52\n",
      "Num timesteps: 252000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.53\n",
      "Num timesteps: 253000\n",
      "Best mean reward: 1.56 - Last mean reward per episode: 1.57\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 254000\n",
      "Best mean reward: 1.57 - Last mean reward per episode: 1.59\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 255000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.58\n",
      "Num timesteps: 256000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.56\n",
      "Num timesteps: 257000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.55\n",
      "Num timesteps: 258000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.56\n",
      "Num timesteps: 259000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.54\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.57\n",
      "Num timesteps: 261000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.58\n",
      "Num timesteps: 262000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.58\n",
      "Num timesteps: 263000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.57\n",
      "Num timesteps: 264000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.56\n",
      "Num timesteps: 265000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.59\n",
      "Num timesteps: 266000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.57\n",
      "Num timesteps: 267000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.57\n",
      "Num timesteps: 268000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.56\n",
      "Num timesteps: 269000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.59\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.58\n",
      "Num timesteps: 271000\n",
      "Best mean reward: 1.59 - Last mean reward per episode: 1.60\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 272000\n",
      "Best mean reward: 1.60 - Last mean reward per episode: 1.60\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 273000\n",
      "Best mean reward: 1.60 - Last mean reward per episode: 1.58\n",
      "Num timesteps: 274000\n",
      "Best mean reward: 1.60 - Last mean reward per episode: 1.58\n",
      "Num timesteps: 275000\n",
      "Best mean reward: 1.60 - Last mean reward per episode: 1.59\n",
      "Num timesteps: 276000\n",
      "Best mean reward: 1.60 - Last mean reward per episode: 1.62\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 277000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.62\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 278000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.62\n",
      "Num timesteps: 279000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.62\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.61\n",
      "Num timesteps: 281000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.59\n",
      "Num timesteps: 282000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.60\n",
      "Num timesteps: 283000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.61\n",
      "Num timesteps: 284000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.61\n",
      "Num timesteps: 285000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.61\n",
      "Num timesteps: 286000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.62\n",
      "Num timesteps: 287000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.62\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 288000\n",
      "Best mean reward: 1.62 - Last mean reward per episode: 1.63\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 289000\n",
      "Best mean reward: 1.63 - Last mean reward per episode: 1.64\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 1.64 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 291000\n",
      "Best mean reward: 1.64 - Last mean reward per episode: 1.66\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 292000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 293000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.64\n",
      "Num timesteps: 294000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.61\n",
      "Num timesteps: 295000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.60\n",
      "Num timesteps: 296000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.62\n",
      "Num timesteps: 297000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 298000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 299000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 301000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 302000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.62\n",
      "Num timesteps: 303000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.64\n",
      "Num timesteps: 304000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 305000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 306000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 307000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.66\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 308000\n",
      "Best mean reward: 1.66 - Last mean reward per episode: 1.67\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 309000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.65\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.64\n",
      "Num timesteps: 311000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.65\n",
      "Num timesteps: 312000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 313000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.64\n",
      "Num timesteps: 314000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.63\n",
      "Num timesteps: 315000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.64\n",
      "Num timesteps: 316000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.65\n",
      "Num timesteps: 317000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 318000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 319000\n",
      "Best mean reward: 1.67 - Last mean reward per episode: 1.68\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 321000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 322000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.65\n",
      "Num timesteps: 323000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 324000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 325000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 326000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 327000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 328000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 329000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 331000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 332000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 333000\n",
      "Best mean reward: 1.68 - Last mean reward per episode: 1.69\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 334000\n",
      "Best mean reward: 1.69 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 335000\n",
      "Best mean reward: 1.69 - Last mean reward per episode: 1.70\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 336000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 337000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 338000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 339000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 341000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 342000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.70\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 343000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 344000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 345000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.70\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 346000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 347000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 348000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 349000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 351000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 352000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 353000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 354000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 355000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 356000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 357000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 358000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.67\n",
      "Num timesteps: 359000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 361000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 362000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 363000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 364000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 365000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.66\n",
      "Num timesteps: 366000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 367000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 368000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 369000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 371000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 372000\n",
      "Best mean reward: 1.70 - Last mean reward per episode: 1.72\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 373000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 374000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 375000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 376000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 377000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 378000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 379000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 381000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 382000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 383000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 384000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 385000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 386000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 387000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.68\n",
      "Num timesteps: 388000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 389000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 391000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 392000\n",
      "Best mean reward: 1.72 - Last mean reward per episode: 1.73\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 393000\n",
      "Best mean reward: 1.73 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 394000\n",
      "Best mean reward: 1.73 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 395000\n",
      "Best mean reward: 1.73 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 396000\n",
      "Best mean reward: 1.73 - Last mean reward per episode: 1.74\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 397000\n",
      "Best mean reward: 1.74 - Last mean reward per episode: 1.75\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 398000\n",
      "Best mean reward: 1.75 - Last mean reward per episode: 1.75\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 399000\n",
      "Best mean reward: 1.75 - Last mean reward per episode: 1.76\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 401000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 402000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 403000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 404000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 405000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 406000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 407000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 408000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 409000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 411000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 412000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 413000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 414000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 415000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 416000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 417000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 418000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 419000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 421000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 422000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 423000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 424000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 425000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 426000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 427000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 428000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 429000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 431000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 432000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 433000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 434000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 435000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 436000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 437000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 438000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 439000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 441000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 442000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 443000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 444000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 445000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 446000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 447000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 448000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 449000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 451000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 452000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 453000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 454000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 455000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 456000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 457000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 458000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 459000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 461000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 462000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 463000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 464000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 465000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 466000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 467000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 468000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 469000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 471000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 472000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 473000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 474000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 475000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 476000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 477000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 478000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 479000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 481000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 482000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 483000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 484000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 485000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 486000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 487000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 488000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 489000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 491000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 492000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 493000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 494000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 495000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 496000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 497000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 498000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 499000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 501000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 502000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 503000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 504000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 505000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 506000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 507000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 508000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 509000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 510000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 511000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 512000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 513000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 514000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 515000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 516000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 517000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 518000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 519000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 520000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 521000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 522000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 523000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 524000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 525000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 526000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 527000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 528000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 529000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 530000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 531000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 532000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 533000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 534000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 535000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 536000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 537000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 538000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 539000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 540000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 541000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 542000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 543000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 544000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 545000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 546000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.76\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 547000\n",
      "Best mean reward: 1.76 - Last mean reward per episode: 1.77\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 548000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 549000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 550000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 551000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.77\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 552000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 553000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.77\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 554000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 555000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 556000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 557000\n",
      "Best mean reward: 1.77 - Last mean reward per episode: 1.78\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 558000\n",
      "Best mean reward: 1.78 - Last mean reward per episode: 1.80\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 559000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.80\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 560000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 561000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 562000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 563000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 564000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 565000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 566000\n",
      "Best mean reward: 1.80 - Last mean reward per episode: 1.81\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 567000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.81\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 568000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 569000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 570000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 571000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 572000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 573000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 574000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 575000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 576000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 577000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 578000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 579000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 580000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 581000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 582000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 583000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 584000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 585000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 586000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 587000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 588000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 589000\n",
      "Best mean reward: 1.81 - Last mean reward per episode: 1.82\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 590000\n",
      "Best mean reward: 1.82 - Last mean reward per episode: 1.82\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 591000\n",
      "Best mean reward: 1.82 - Last mean reward per episode: 1.83\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 592000\n",
      "Best mean reward: 1.83 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 593000\n",
      "Best mean reward: 1.83 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 594000\n",
      "Best mean reward: 1.83 - Last mean reward per episode: 1.83\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 595000\n",
      "Best mean reward: 1.83 - Last mean reward per episode: 1.84\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 596000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 597000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 598000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 599000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 600000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 601000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 602000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 603000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 604000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 605000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.84\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 606000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.84\n",
      "Num timesteps: 607000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.84\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 608000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.84\n",
      "Num timesteps: 609000\n",
      "Best mean reward: 1.84 - Last mean reward per episode: 1.85\n",
      "Saving new best model to /tmp/gym6_2o1o1/best_model.zip\n",
      "Num timesteps: 610000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.85\n",
      "Num timesteps: 611000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 612000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.84\n",
      "Num timesteps: 613000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 614000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.84\n",
      "Num timesteps: 615000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 616000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 617000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 618000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 619000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 620000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 621000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 622000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 623000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 624000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 625000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 626000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 627000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 628000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 629000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 630000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 631000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 632000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 633000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 634000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 635000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 636000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 637000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 638000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 639000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 640000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 641000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 642000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 643000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 644000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 645000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 646000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 647000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 648000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 649000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 650000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 651000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 652000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 653000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 654000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 655000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 656000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 657000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 658000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 659000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 660000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 661000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 662000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 663000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 664000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 665000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 666000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 667000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 668000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 669000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 670000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 671000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 672000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 673000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 674000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 675000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 676000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 677000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 678000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 679000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 680000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 681000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 682000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 683000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 684000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 685000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 686000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 687000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 688000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 689000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 690000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 691000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 692000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 693000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 694000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 695000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 696000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 697000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 698000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 699000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 700000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 701000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 702000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 703000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 704000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 705000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 706000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 707000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 708000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 709000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 710000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 711000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 712000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 713000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 714000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 715000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 716000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 717000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 718000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 719000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 720000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 721000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 722000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 723000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 724000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 725000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 726000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 727000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 728000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 729000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 730000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 731000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 732000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 733000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 734000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 735000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 736000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 737000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 738000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 739000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 740000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 741000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 742000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 743000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 744000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 745000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 746000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 747000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 748000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 749000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 750000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 751000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 752000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 753000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 754000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 755000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 756000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 757000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 758000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 759000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 760000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 761000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 762000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 763000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 764000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 765000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 766000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 767000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 768000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 769000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 770000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 771000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 772000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 773000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 774000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 775000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 776000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 777000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 778000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 779000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 780000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 781000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 782000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 783000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 784000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.84\n",
      "Num timesteps: 785000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 786000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 787000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 788000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 789000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 790000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 791000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 792000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 793000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 794000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 795000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 796000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 797000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 798000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 799000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 800000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 801000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 802000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 803000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 804000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 805000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 806000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 807000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 808000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 809000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 810000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 811000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 812000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 813000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 814000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 815000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 816000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 817000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 818000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.83\n",
      "Num timesteps: 819000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.82\n",
      "Num timesteps: 820000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 821000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 822000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 823000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 824000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 825000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 826000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 827000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 828000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 829000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 830000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 831000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 832000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 833000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 834000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 835000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 836000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 837000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 838000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 839000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 840000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 841000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 842000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 843000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 844000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 845000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 846000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 847000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 848000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 849000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 850000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 851000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 852000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 853000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 854000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 855000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 856000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 857000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 858000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 859000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 860000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 861000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 862000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 863000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 864000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 865000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 866000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 867000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 868000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 869000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 870000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 871000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 872000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 873000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 874000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 875000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 876000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 877000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 878000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 879000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 880000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 881000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 882000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 883000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 884000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 885000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 886000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 887000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 888000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 889000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 890000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 891000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 892000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 893000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 894000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 895000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 896000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 897000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 898000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 899000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 900000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.81\n",
      "Num timesteps: 901000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 902000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 903000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 904000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 905000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.80\n",
      "Num timesteps: 906000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 907000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 908000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 909000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 910000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 911000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 912000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 913000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 914000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 915000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 916000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 917000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 918000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 919000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 920000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 921000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 922000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 923000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 924000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 925000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 926000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 927000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 928000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 929000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 930000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 931000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 932000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 933000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 934000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 935000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 936000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 937000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 938000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 939000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 940000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 941000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 942000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 943000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.69\n",
      "Num timesteps: 944000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.70\n",
      "Num timesteps: 945000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 946000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 947000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 948000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 949000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 950000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 951000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 952000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 953000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 954000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 955000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 956000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 957000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.72\n",
      "Num timesteps: 958000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.74\n",
      "Num timesteps: 959000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 960000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 961000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 962000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 963000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 964000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 965000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 966000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.79\n",
      "Num timesteps: 967000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 968000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 969000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 970000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 971000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 972000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 973000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 974000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 975000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 976000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 977000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 978000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 979000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 980000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 981000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 982000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 983000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 984000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 985000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 986000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 987000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 988000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 989000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.78\n",
      "Num timesteps: 990000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 991000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 992000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 993000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 994000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.76\n",
      "Num timesteps: 995000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.77\n",
      "Num timesteps: 996000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 997000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 998000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.71\n",
      "Num timesteps: 999000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.73\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n",
      "Num timesteps: 1001000\n",
      "Best mean reward: 1.85 - Last mean reward per episode: 1.75\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model = PPO(\"MultiInputPolicy\", env).learn(total_timesteps=ts, callback=callback)\n",
    "end=time.time()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f1ecdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 31.309033834934233 min\n"
     ]
    }
   ],
   "source": [
    "print('Total time taken: {} min'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb5e9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97681657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e578e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "808e52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "ans=[]\n",
    "while i<1000:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones,_, info = env.step(action)\n",
    "    ans.append(obs)\n",
    "    if dones:\n",
    "        break\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a31cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf4cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Cantilever beam design:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(ans[-1]['macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18698f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Cantilever beam design:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMwAAADNCAYAAAD0fp9XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAexAAAHsQEGxWGGAAAGe0lEQVR4nO3dwUuUeRzH8W+6BOLeaj2EgnRLkA3pD5AOiYriToMePBiJu5ISGYKEdAgUMciLB2WHtgjs4GGQMRHNgxehmx2WvIgYSYcNCw/DJqTP8jywRsiUHxmf+T0+7xcMyWM+fpF58xtHnud3xvM8zwAcSdHR/hsAH8EAAoIBBD/ZMf3W8LNVVhz7ywHnvf1w1dLp9DfHjv2M92N59OCXfMwFOKn/YeWhY7wkAwQEAwgIBhAQDCAgGEBAMICAYAABwQACggEEBAMICAYQEAwgIBhAQDCAgGAAQc7rYbLZrN26dcvOnj1rtbW11t7erpwXiNcK419plkwmLZVKWSaTCXcqIGorzNbWllVXVwcfFxcXHxxfWFgIHpvvvoQzIRCFFaa8vDyIxre/v39wvK6uzsbGxrieH7GU81mfSCSst7fX5ubmrKmpKdypgKgFU1paak+ePAl3GsBxvK0MCAgGEBAMICAYQEAwgIBgAAHBAAKCAQQEAwgIBhAQDCAgGEBAMICAYADBqboKrO7CZXPNwvvXhR4BecQKAwgIBhAQDCAgGEBAMICAYAABwQACggEEBAMICAYQEAwgIBhAQDCAgGAAAcEAAoIB8hHMxsaGdXZ2BvtcAvhBMBcvXrTHjx/n+jQQS/IlymwKiziTf4dhU1jEWc5gtre3rbu721ZXV21kZCTcqQBH5Vwmzp07Z5OTk+FOAziOt5UBAcEAAoIBBAQDCAgGEBAMICAYQEAwgIBgAAHBAAKCAQQEAwgIBhAQDCA49lVg6T/L7O/Ur8f9csD5DX2r7xw+xgoDCAgGEBAMICAYQEAwgIBgAAHBAAKCAQQEAwgIBhAQDCAgGEBAMICAYAABwQACggHyEczMzIx1dXVZW1ubLS4uKucE4nfFZUtLS/D49OmT9ff327Vr18KdDIjiJcpDQ0PW09NzaFPYfy170rMB0XlJ5nmeDQwMWH19vdXU1BzaFLbESsOaEXB/hRkfH7elpSXb2dmx9fX1YINYIO5yBnP79u3gAeAr3lYGBAQDCAgGEBAMICAYQEAwgIBgAAHBAAKCAQQEAwgIBhAQDCAgGEBAMEAYm8K6aOH960KPgFOOFQYQEAwgIBhAQDCAgGAAAcEAAoIBBAQDCAgGEBAMICAYQEAwgIBgAAHBAAKCAQQEA+QjmLW1tWATpWQyaRMTE8o5gfgFc+nSJZucnLTp6WlbWVkJdyogii/JMpmMNTY2WkNDw8Exf0PYu3fvsiksYum7wTQ3N9v8/LxNTU0dHGNTWMRZzptgLC8vWzqdtt3d3W9WGCDOcgZTW1sbPAB8xdvKgIBgAAHBAAKCAQQEAwgIBhAQDCAgGEBAMICAYAABwQACggEEBAMICAYIY1PYxO//2KMHbMIaRXUXLhd6hMhihQEEBAMICAYQEAwgIBhAQDCAgGAAAcEAAoIBBAQDCAgGEBAMICAYQEAwgIBgAAHBAPkIJpvN2pUrV+zFixfK+YB4BjM6Omqtra3hTgNE8RLlly9fWlVVlX3+/PnQ5/xNYf3H5rsvYcwHuB+Mv7+l/5LszZs3VlJSEuxxWVRUdLAprP/o++OvsGcF3AxmeHg4+Pfp06d2/vz5g1iAuPvuXWNu3LgR3iRABLB0AAKCAQQEAwgIBhAQDCAgGEBAMICAYAABwQACggEEBAMICAYQEAwgIBggjE1h3364av0PKy0fNjc3rbIyP+fKl9M8U/Udy5vNU/xz8s9ziOeAvr4+zzXMdDR9MZvJiZdk/iXPrmGmo6mL2Uxn/GpO7OzAKePECgNEBcEAUQnGv5VTR0eHdXV12dTUlLlgY2PDOjs7LZlMmitmZmaCn1FbW5stLi6aC9bW1qy7uzv4OU1MTJgrTvyOrV4BPXv2zMtkMsHHra2tnkuuX7/uuebjx4/ezZs3PZfs7e157e3tnivu37/vjY6OerOzsydy/oKuMFtbW1ZRURF8XFxcXMhRImFoaMh6enrMFZlMxhobG4MbPbrg/zu2lpWVndj3KGgw5eXlQTS+/f39Qo7iNP+NzIGBAauvr7eamhpzRXNzs83Pzzvzctq/Y+urV6/s+fPnlkqlTuQ5dey/9OdDIpGw3t5em5ubs6amJnPB9va2DQ4O2urqqo2MjNi9e/cKPZKNj4/b0tKS7ezs2Pr6evC7gwtPznQ6bbu7u86sMGHcsZW/wwAC3lYGBAQDCAgGEBAMYEf3H5iUp4zgaU/lAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 320x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(ans[-1]['micro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2fa8fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwgAAAC+CAYAAACRfijiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABjiElEQVR4nO2dB3gUVdfHTxJSCYSW0Kv0jg1pgoqi8toQEETBLr2KHXnVT0CsCAiI72t5xQIIInZA6YiCdKSjKNJ7DSX7Pf8b7nB3MtuSTbKb/H/Ps9ns7OzM3Zm7955z7ikRLpfLJYQQQgghhBAiIpG53QBCCCGEEEJI6EAFgRBCCCGEEGJBBYEQQgghhBBiQQWBEEIIIYQQYkEFgRBCCCGEEGJBBYEQQgghhBBiQQWBEEIIIYQQYkEFgRBCCCGEEGJBBYEQQgghhBBiQQWBEEKIRatWrdRD88cff0hERIS8//77udouQgghOQcVBEIIyUW2bt0qjz76qFSpUkXi4uKkcOHC0qxZMxk1apScOnUqW865fv16+fe//62E/7zC3LlzlSLj6fHpp5/mdhMJISRsKJDbDSCEkPzK119/LR06dJDY2Fjp2rWr1K1bV86cOSMLFy6UwYMHy7p16+Sdd97JFgXh+eefVysFlSpVcnvvhx9+kHCmb9++csUVV2TY3qRJk1xpDyGEhCNUEAghJBfYvn27dOrUSSpWrCg//vijlC5d2nqvV69esmXLFqVA5DQxMTESTpw+fdqtzS1atJD27dvnapsIISTcoYsRIYTkAiNHjpTjx4/Lf/7zHzflQFO1alXp16+f+v+9996Ta6+9VlJSUtRqQ+3atWXcuHEZPoPVgH/9619qBeLKK69ULktwXfrwww+tfRBLgFULcM0111guOHDRcYpB8MSGDRuUIF6sWDF1nssvv1y+/PJL6/1ly5ap437wwQcZPvv999+r97766itr286dO+WBBx6QkiVLqu9Yp04d+e9//+voRgR3oWeffVbKli0rCQkJcvToUQkEHKN3797yxRdfqFUbfb7vvvvO2mfq1Klqv3nz5mX4/IQJE9R7a9euDei8hBASLnAFgRBCcoGZM2cq4b1p06Y+94UyAAH21ltvlQIFCqjP9uzZU9LS0tRqgwlWHiC4P/jgg9KtWzclZN93331y2WWXqWNcffXVyg3nrbfekqefflpq1aqlPqef/QGuT4iTgID+5JNPSsGCBWXy5Mly++23y+effy533HGHUhjw/bAd7TD57LPPpGjRotKmTRv1es+ePXLVVVdZgntycrJ8++236jtA+O/fv7/b51988UW1avDYY49Jamqq2wrCsWPHZP/+/RnaXLx4cXV8DZSoadOmqetYqFAhdT3uvPNO2bFjh9q3bdu2kpiYqNrfsmXLDO3HtYRyQQgheRIXIYSQHOXIkSMuDL+33XabX/ufPHkyw7Y2bdq4qlSp4ratYsWK6rjz58+3tu3du9cVGxvrGjRokLVtypQpar+ffvopw3FbtmypHprt27erfd977z1r23XXXeeqV6+e6/Tp09a2tLQ0V9OmTV3VqlWztj311FOu6Oho18GDB61tqampriJFirgeeOABa9uDDz7oKl26tGv//v1ubenUqZMrKSnJ+v5oL9qC722/Jvo9T49du3ZZ++J1TEyMa8uWLda2VatWqe2jR4+2tnXu3NmVkpLiOnfunLUNx4mMjHS98MILGa4dIYTkFehiRAghOYx2iYHl2h/i4+Ot/48cOaIs5LBqb9u2Tb02gfsR/PA1sMbXqFFD7RsMDh48qGImOnbsaFnr8Thw4IBaEdi8ebNyFwJ33XWXnD17VlnqzSDow4cPq/cA5HWsOtxyyy3qf308PHA8fL/ffvvNrQ1YkTCviclzzz0ns2bNyvCAK5RJ69at5ZJLLrFe169fX2WQMq8T2rh3717L/Uq7HmHlRrefEELyInQxIoSQHAaCKICA7Q+LFi2SoUOHypIlS+TkyZNu70GATkpKsl5XqFAhw+fhznPo0CEJBnBhgiA/ZMgQ9XACQjXcjxo0aCA1a9ZULjlwFwL4v0SJEiqmAuzbt08pDMjW5CljE45nUrlyZY/tq1evnhL+feHPdbrxxhvVtUWbr7vuOqv9DRs2lOrVq/s8ByGEhCtUEAghJBcUhDJlyvgV5Io6CRBOIWi//vrrUr58eeVz/80338gbb7yhrNkmUVFRjsdJ96zJOvp88P/XMQROAdYaWNpfeukltSKAFRMEMnfu3FnFUpjHu+eeezLEKpjWfRNPqweB4M91QvAy4iqmT58ub7/9toqVgLI2bNiwLJ+fEEJCGSoIhBCSCyDbECzmWBXwlqMfAckIxIVgbVq9f/rpp0yf2wzWDRQEHoPo6Gi/LPVQEFBzAW5EyFAE9yqkdzVdoKA4nD9/3q/j5TRoPzIxzZkzR37//XelQNC9iBCS12EMAiGE5AKPP/64yv7z0EMPKcu008oBqilrS7dp2YZbEVKfZhacF8C1J1CQahVpUJHqc9euXRneh8uQCbIjwe0Hrjl4IKUrMilp8P2QPQgKhNOKiv14OQ2UFsQv6PYjfaw3FydCCMkLcAWBEEJyAQTIfvzxx8oaDSHarKS8ePFimTJlikpPOnDgQOVShCDeRx99VNVOmDhxohLUnQR0f4APPQTzl19+WSkbcKXRdRb8YezYsdK8eXMl+D/88MNqVQFKDlZD/v77b1m1apXb/viOCB5GvQTEIkRGutumRowYoVZEGjdurI6HQGsEQyM4efbs2ep/f1mwYIEqnubkpmR3VfIHrJS0a9dO1V44ceKEvPrqqwEfgxBCwg0qCIQQkkugrsHq1avllVdekRkzZqh6BxDWIci+9tprSljGa2TOQWEw+P2XKlVKevTooVxzUFgsM+AY48ePl+HDhyuBHe49END9VRAgwKMQGlyHUHgNGYzw2UaNGilFwA4UBLQfAdZO7jlwPfrll1/khRdeUBmP4O+PWgSoNQAlJhBQz8AJBHlnRkHQ7X/33XeVaxayNxFCSF4nArlOc7sRhBBCCCGEkNCAMQiEEEIIIYSQ4CkIWJpeuXJl0HJsE0IIIYQQQsJIQejfv7/85z//sZQDVPO89NJLVW5us9okIYQQQgghJB8oCAiWQ3VMnZ97+/btsmHDBhkwYIA888wz2dFGQgghhBBCSKgqCKiGiQwYAJU8O3TooErOI5vGmjVrsqONhBBCCCGEkFBVEJCObv369cq96LvvvpPrr79ebUf6Ok+l6wkhhBBCCCF5tA7C/fffr/JAoxomckKjyiRYunSp1KxZU3KStLQ0+eeff6RQoUKqLYQQQgghhBBnUN3g2LFjUqZMmQxFK7OkIPz73/9W1T7/+usv5V6EIj4AqwdPPvmk5CRQDhAcTQghhBBCCPEPyPHlypXLm4XSjhw5IkWKFFFfsnDhwrndHEIIIYQQQkKWo0ePKuP64cOHJSkpKWsrCJ5K1zvRt29fv/cdPny4TJs2TWVBio+Pl6ZNm8rLL78sNWrU8Ovz2q0IygEVBEIIIYQQQnzjyzXfrxWEypUru73et2+fCkqG9R5AC0lISJCUlBTZtm2b+MuNN94onTp1kiuuuELOnTsnTz/9tKxdu1YFQRcsWNAvLQjaD1YSqCAQQgghJBQ4dz5Ndhw8KRWKJUiBqCzXpCUkaPgrO/vVa1HrQD9eeukladiwofz+++9y8OBB9cD/KJb24osvBtRIZEG67777pE6dOqq2wvvvvy87duyQ5cuXB3QcQggh4StIbdt3XD3n9vGz2hb75zNzvNNnzsm8jXvVc17F23XJ7v6QmXMF2ibs1+7txXLta/PUs6/+kJPfmRB/CThIeciQIapYmukGhP/feOMNad++vXTp0kUyC7QZUKxYMcf3U1NT1cPUgggh4UW4WtbCtd2hdg30Mcokxclfh07JwMmrZM3OI1K/bJJM69k0y8c126YFtdU7j0i9MoXllQ4NJCoyQiqXKKj2MT8DzH1fv6uhlC4cK8t3HJbGlYtJXIz36dI8F77Lxw9dKXeOXyIb9xx3+26e2oltKYkxcsWwH+XU2fMSVyBSpnZvIrHRUVK+aLz8c+S039c9t/oqzrt9/wn1v77GGig8S7cflMsqFJG73/3Fuk7mddmy97j0+WSFbN57XKqnJMqXvZtZ112///ehk1K2SLxUK1nI7fjmd8b/OJe3+2berxopiTLDOJen/dDeyY9epfqt03fUoB3YH+BZXxN7Xwd4b8BnK2XtP0elXtkkmW78Brzdx8y+R4i/BBykDFeiefPmKbcgk19++UVatWqlXI8ym7L01ltvVe5KCxcu9JhB6fnnn8+wnS5GhOQsmZ2A/J2Us3KOQPB3ksX/t41dlEHY8+eYgb52Og4I5rUIRLgwBfqOE37OINj5OqYnhSA+OkoJwiY/DmopVZITfQqbTm3U/apu6UIyqE0NuaJiUSXcd3vv1wzfv1pyQXnjroby1PS1qi1QCPq1riYPfei+eg0PXUyQUREir3WoL9FRUUrBqFg8QQmoQH83CKTmubANQr3mv90ul6IJMdL7kxWy8/ApqVQ0XsZ0aSSViheUduMWy6a9JyQpRuTImYz3K7ZApKSeS5PqyQVlVOdGGYRjEwji/xq9ULbsOyF1SheSGb2bZxA4ddv0MxQTuyLkq//Z7w+4bcxCWbfrmPq/bpnCSsHBPT9zLk3aj1+i7ndMVIScOX9R7Jg14Go5n+aS3h8vly373OWHMkmx8k3f5rL76Bnp+dGvsu3AxeuJ43/Rq1kGIbtOqUTZduCUOhf62JInW8lvO45I6aQ4t+sGiz0s/Bf7RIK8fc/lljKmr03q2fNy01sXZZJKxeLlj4PpCgKUmGk9msje42fcrukfB07KHW8vVm2IjYqQCsULKqXHBP3hjdmbVf8zeeeey+TaWinqf92ncd+hsKLt6H9oo/4t6v7epEpxde9w/28ds1D1J/Tr6ReuUaDjh9PvD8devPWApBSKdVRcvSm/3sYafe0yM755GydI1lyMAlYQbrnlFtm5c6e8++67yq0IwCXokUcekbJly8qXX34pmaFHjx7y7bffKuXAU9olpxUERGJTQSAk57Bb1JyEZU9CJgSB69+Yb+1Xo2SifN23hePnvZ1DWyM9CTT+CN6mwKsnYEx6WtgxrckQzjYZE/zL7erJbQ3LZFBu1OQ8dpFs2nPcEh5Maymsjx0m/KyEAgg4L99ZXwZMXqn2t1tTzfNLRIT6jBYQ0UZtxfQ1SdsFPUymnqz2Thb3wVNXq30rF0uQ7QcvCnDvdr1MWtVI8XpMXA+tWMVHR8qps55dKOqWKSSDbkgXdPDZO8YukjX/pK8Sa8uqp3Ph+5nCnomv82aW2qUSBYeF4BcdKer/2CiRVHedJ1soXyRORnVqKAdPnpXkxFjZffS0YCYvVzRe+kxaJtsPXZwnX7ilttzasLQSkl/7YaMS4OMKRMjpcy7r2mhFCMI7+mzVlEJWP4Uyhd8FhG/0vxF31peVfx2W9xdvlx2H0gX2SsUSZGT7utLxnV/c2lmhaLzsuNBPncCxIyVNNu7zvI83htxcS/6zaLubImZH3xt1vpSCMrN3c/X/gs375cWv1mdon1Zc9XOBCJFzXqQkffzoqAg5e96lVn5On/Pe3+KiI+W0lz5ZuVicDL6xlvT8eIXj++WKxMrfhy/eY4B7+HjrqvLhLztk19GLWuaXvZrJoCmrVD+tUiJeHm9TSylLus9oZRe//bkb9ynlFStnY+dutZQ9XLf/PXi5tHh5nnUtze+B+4ixbOCUVUo5qlgkTp5uW0uaVS3hcbXI/B1rpREK7WOGsqM5fuqMfL1mt7StV0ptNw03t4xZKJv3pisI5YvEyrNta6vz7jqaquab46fPyder/5Fe11aRxNgYNW80KFdY/R60omMqFpkxTJ3z4zNO4zAU573HTlvf12nsxn74HloxdFq9MudDbPOldGWbgoAA5W7duqn4gejo6PQvfu6ctGnTRsUQIFA5UHr37i0zZsyQ+fPnZwiI9gaDlElet7bn9DKx6erwyx+H3KxuehDCoGpa1D64/wo1IDsJmXahuGqJBNlxGBZFdwsi0IMgBj67wId9tMUW7Wj04mzLDQOCIz5jWrdxTvM13D1gITXdG+wCrwbC6Mg767l9RycgXK0Ycr01sGMg7/7Rctm6L32yAuWLxspfhrD23n2Xy/3vL/N4TFjQcb8XbdnvaPnWaAEP6MkVlsT+11eXN2dvVsIcVmg+e7SxtB+3WLbsPyWXFI9XbdWTvnltMfHAMngy9Zz0/nSl+EutUokCYzAUHJP7m1RUE/LdE39Rlkx/iC0QIannXOr7jO7UUB6d5C4cfdevhTw+dbXluqHpf21VuaZmsjw9fW2G76aJQv/y+1sRCFp/2QTQcAQjkl0MLx4fJUdTz7sJuvkdKBx7jqYG/ZqYyhl4oGkl6XlNZXng/d8y/I5NTEUV4ylWxPSMUb4oViJPS6lC0VKgQJT8fUFJ9Qf76pU55n/y0JVqznt91qZ0d68y6W6GTquXcHXbdeSUNCqf5KZ46xUb+/7r/zkqPSb9ppQvrHAdP3Ne/rywCgWwyjSqUyMZ/eNmdRysZg27o770+2ylm/JrjmNQpka2byB/HjghAyavUkYszIdVUxJV+7WCa67U63kKMvMVNcoHV0HArqg5kJycLH///bcKTgaooFy9enV/D+N2vD59+sj06dNl7ty5Uq1atYA+TwWBhALeBHnTgupkrfVk4fZm5fV1Xn+XeZ2sE1qoNoGl+9OHG8vlL81RljFY1ConY8n8hDUI6TbimDNW/iNPTFvjZmm2u25oShWKkYKxUbJ1/8XBEgPfax0bymNTVqmBTm/TliW7KweonlLQTRCF0mLuo6172lXDF7CYaatU2cIxEgWrjMNE9Ny/akmpwnEy4rsNssMY8J1IToyWR5pXkZe+2+j4Pu7KD4Oay8Pv/ybbD2TOVdPpmL6+bcnEaDl8+pwSzkOJmEiRM0bjqxSPV64jntBWcEJI3sA+BmQnBSIj5FxaxhGkxgVDy8/bDikBf9LPf3gdh2AEwjwFo8nsdbvly9U75Y+D/isx2UH1kolqJanD+CVqVTYt9aT89WbH4CoIiBOIi4uTdevWBSzMO9GzZ0/5+OOP1eqBGfQMoR91EXxBBYFklkCXBD0J2t5cYaAc3DRqgZuwp32szc/Bkj2zTzNJjI9x2+7JN9ubL7/TdtNdw/RJNV04tJvAHw4WdW2lffPHLW7byhSOlX+OXrQyYgBqP36xstBoYQ2WECgQ9uVwf6hYNE7+tAnlUEgWPH61NHt5nkdBP6ZAhMwffLV0eudXtdwdqCB5SYkE2bo/OAI6IYSQ/EUBzNESWlxdrZjM33xQ/e+vghCQ30JkZKRSDA4cOJD11orIuHHjVAMR3Fy6dGnr8dlnnwXl+IQ4ASEawjFcWPDsKdWePU2d0zZY4M1sFVAe9OdvG7PITTmA9Rp+gfYsF3BzafR/s5Wfpbldg2VDBMpB4di855is23lEpiz7y9pv497jKihtw66jsmDzPrftWL3AcqhWBPCM5c5JP//pphwAKAdooxN25QCYykG5pBhpN26RtXyrAzs37zuZKeUA2JUDAGXj1tGLlXIAV4EYrLnagPvSVcPnOSoHum3eoHJACCEks4SacgC0cpCtaU5HjBghgwcPVsJ93bp1JSsEGP5ASFCAUG8KzHit/dvNfexp6uCGY26bv2mf8lfUIChVKwDYHwK6CYRaBBjBpxHBScjQ8c+RdOEZ7i/ws/yqT/MM2V3g1gN/eE/+kwDuNTeOWpBhO1ybths+8QBBtJ5AG6Ej+OGF48bfDqlXPDQ1y2jF5MApepUTQggh2UHAkY9du3ZVKU1R2AxuQKhZYD5I/iQUC71ktk2w5D/0vns2DsQDQPiHGxGApf2BD5ZZPvIAWW7gww9L/8DPnAM9T505Lze+MVcJ81o50CB13uKtBzOkftR4Ug588fxXawPaP1DlgBBCCCF5i4BXEN58883saQnJ02kvAz1eINl7fBVIsrcJFnxkLdCpJvX+OlPPpf83O4Mwjn2RTQDZcW58c76VB9sOzocgWrv7jgZuON4E8B6Tgl9FHDnECSGEEEKyTUFAilNCvFWNxGszoDY7lQ1PwbpOlSy1GxGOh9SYOlMQ8vLrcyE3tpOlHisGSP0Jv3ZTOShdOMYt53Tl4gVVKk0zC04g1vnscsshhBBCCPGXLCVXP336tMokZD5I/gICOtJkwiIP6pdLsgqBBEvZ8Hd/HZSLNsEdCMG9GlTZNF2NdEVKXcUSx1C5ij9a7tE3H9VOe3/8m9t2UzlAKs3tB05I21ELZJufud8JIYQQQsJeQThx4oQqbIaCaAULFpSiRYu6PUj+QVvvVWVcl0sVXJrWw7fF31tcAJSL+h6UDafP4n2sHJhBuZ8v/1utDpjVLBErgOJT9s/qc6GK651w//FiwUcg8BZbwK8JAo3Bn4dPsygTIYQQQsKWgCsp9+rVS3766Sd58cUX5d5775WxY8fKzp07ZcKECSrDUZcuXSSnYB2E7EWn8QT2aoIAwrpZ7dZeUdfpePZ8/E776pgCXSocz8j+owuH4bOvdGhgVd3F/sjMY1ZzRWVCVBTUlVV1BiB8dkr3JlYJcoDvuGTrfnnuy/VBunKEEEIIIaGHv3UQAo5BmDlzpnz44YeqdsH9998vLVq0kKpVq0rFihVl0qRJOaogkOzDFOYBXIjgt28K9NoCD/ccpOZE5Vrtyw/sgcP+pBcF2B9Kga5ADFchczUAn9UpPXXV3RibopF63iUdrygvjcoXle4fLbMyBuGzN7wxT1XFrVu6kHz6yFXSc9Jyx3gBQgghhJD8SMAKwsGDB6VKlSrqf2geeA2aN28uPXr0CH4LSbbhLVsQtpuZeNYYtQDM/V/r2ED+PnRS7n9/mVtA8KDJq3wGGiN2ASBzEDL/NK5cTO2Hz5tCu6kc2MEKgVP+fzD0y/USHSly1vZxKAdg7a5jcu2rP8neE6FY1oQQQgghJEwUBCgH27dvlwoVKkjNmjVl8uTJcuWVV6qVhSJFimRPK4lfgr0vlyB/sgXpYyNjT/WUgsrvXlvqe338m3Ljwf5I94mc//g83HZ02lDEDQAz0Bi+/3A9Kl803i27z6DJK2Vk+wbSfvxiOXU2TeIKRMglKYVknYcUoZnBrhzYoXJACCGEEJJFBQFuRatWrZKWLVvKk08+KbfccouMGTNGzp49K6+//nqghyMB4k2w9+US5CtbEBQOfez46EgltFdLLihv3NVQBkxeaQn2eH/hlv3W53FOBCjr1QWgXY+QHhSuR1Awzqa53Fx5YP1vO3qh9fr0OVdQlQNCCCGEEJIDWYwGDBggffv2Vf+3bt1aNmzYIB9//LGsWLFC+vXrl4km5D+yUnXYLtjrzDxOLkH2rD0mZgafemULK3efzXuOWceGcgA27zshu4+ezuCj32vSb1aRMXweIG4A7cA5R7avLxWLxav0oFoZMIOICSGEuIO6KoQQEpYrCKh9EBcXZ71GcDIeJGeqDnsKDIbLD1x9tJJgf09n7dHnwjPOvWXvcen76QqVqhSZfpxIc9AxEAT8SIsqkhhXQF77YaP6PFYLoBDo1QdCCMmvYKTVo2CBCPGaQvn5W2tL00tKSOnCsXL5S7PVaqompWC07D1xNvsbTPIlpQpFy+5j7F8kCCsIiDO4+uqrZciQITJnzhw5depiVVkSvEJg3lYZEBj83n2Xy6mz563jQAFA+s7/drtcJt57mdt7t45ZqNKR3j5moWzYddR6KOXgkxWWZd+pgjCIjESdgIypsPp9tlIe+GCZlUpUrxZQOSCE5HcwCmqTizflAIaZuy4vrzK6JcbHyPSezdzef/e+K91e10jJfCFKEnw82NX8Bh8f2a6O1VeyCyTssAOjHpQDPPtDuSIXjcN2KhaNl9qlLtYkChXKJcVIr5bpiXVINisIs2fPlhtvvFGWLl0qt912myqOhgxGzzzzjMyaNSvQw+U7vBUC0+h4Agj1eNZKglmY7PUfNrlVL4Z7D4KGIbCPmr1JWfH15KMDjddeyPhjPjbt9e72ExcdKQ99uFwNXu92vcztvYAKaBBCSBjx9t2NpGpy1gQepzGyf+uqbq/fvKuhMvDocR4JJrACDPAcaZMcB99YS/IyZZJipY6DQcoTMZEiyQWjs6095Yp6FoqH3V5XapYq5FEA97ePPD5tXZbnUyQW8QQMiiuGXC+Vi7vLG9qoh2f9HpKTTOhyaYZj1CiZKOPvybhd8+ehU3LyTLphMpBLUbpQtHzVu5l8169FphSMSkVjpaLR9hk9m6pkKJoiBeOkz3XVpGqJ8FSsi8SljwV2iic4bw8mAXdpKANPP/20/PDDD3L48GFVNA11EEaOHKkUh/xEZmIJtGvPj4Naeqw67FQvwL76gO2vd2xgHQcTjH4PioC24ntaFfCX0xeOo9tzSYn4LB2PEELCgSrJiTLunktVAoaVQ67LIFwFgpbxYcx5qFllSwFAQcc3Zm9WxiAYfzCXYCzXK8B43nc8vYaLplzRi0amWPguhQFFYvxrJyzZqFmDWfHrPs1VZjtfTO3RTH56rJVHF9msMuGeyy1jnEl0VIQ8/cVaawUdU2UwmoC6P4jfMynuoABVKBInVZPT+yQMgnuPn3E7BhKD6BjBFtWS1erUt/1aKEFfbzeNjHgP8sQ3/a6W62qXtPpY3TKFpHrJRFWT6Klpa63P1Cmd6NZO3Ls/DqZ7lNglonKFY9xWSGB41J+ZNbCV1C1XRGqWLixf9mmhFIWyDisV/a+pKm93bugW+4jf5uzHrpU5A1tabW9Qoai83eWiMXPtP0fVb+qrvi3U9wgn6pYpLAufuNZN4QFY8JnY9QpHz44J91xq9Yscj0EAmzZtkrlz51qP1NRU+de//qWKp+UXshJLgP0w+XhKW2pXOM6cS5N5G/fKZRWKWPEHOjBYfwYrCJh0MKGg8ziVDsBkkuqw1u1UK8AJrCRUKRanAul2Hb04GBFCSHZQrkis/H3YXUDOTAyANzD5RkiE22oqtg2eujo9dfOFOK53ul6uDEKPfvRbQO2AYjG9ZxM5ePKcNV6vGNJaFm89IMO//V02XzgvxvX5m/ZJmSLxbmmjm1Qpbr3GMyrEY77BnFEsoYBcOezHgAxBUEoQQ5aTHD7j+3z9rq0qo37cYhmk9h9PdYvF8MSBE6lSv3wRWT30BnVNX5+1SQmFgXxPCFr9W1dTc5wJBO9LkguqjIC4N1ih15x1OHZWLisE+sfa1FD3GzLAZS/NUZZ9CNHf928uLUbOT5/fI0TKFo2XPw+eUnGHWB3QdYgAlJSFT7SUIgnpSUPssYdj7r7UWqkC5j5aLsH54cqs/7/prYUeMxbCgGnWQgLVUxIlNjpK9Vn0/5m9093mvl6zW9rWK6XOpWsfxcVcFEOxHYrC9/1ayKX/N9vq17g/vVtXU+/fULe0Y5p3vNbb8d20rKQ9NbDvN31bWGncf/njkKQUipUnPl9tKXm4B72vqSY9Pg7sNx5sxnZuJAmxUaov4Pp82+9qlXQGcaUA8l278T+r9sKz483Zm1Wfh/LXsnqyXFerpHIh7/PJbyrBTLXkBEHZqa37nV3aPRHhcrkC6tJly5ZVcQdQBvBAutP69etLRETOWzKOHj0qSUlJPstFZweYKGD10UB7NTuov8qCqWjgx/5Khwby2JRV6maDmiULyp8H0y1KUAB+ffpa2XU0VQZOXmVNXnqyMNvjFBTnSXEghASPaimJltBnUr5IrPyVSWG361UVZMGmvbL9YHqRPw1W9CAoePtdj7v7Uhk7d4s1ppjocQL27JF31pNh322QAzkcEAuXEl3p3M64LpdKj0nBn6xhmYbwCZeEL3s3V9t05XZMsnD70UIRwDa8hzH6fFqarN+dfn/tVd5NYMR5vWNDjzVp7HOICQQipJfWn/VU+8bpGPgspnUt9JhAmGhetYT8deiUylynlaBgUaZwrPxz1Hcfn/poE+n7yXL5x2ZoghSBaUvPdXe/+4u1Mu4EhOdVz13vJmTqekD4fhBc7UK/3SAGZXBm7+bquuq52AQCMeJD7KnETWDx3nnY/bdp/04Q0gbdUFPKFY1X8hJShyP+D31wVKdG6hz2WkTLdxy2hOjjp87Iv0YvtKz0Zvse+WCZbDfiGSGUzxrYMkOdJH+MmnaZBG3XYwcE/y97N3O73vbP4Lcyo1czt99UZhKyoIAqFL7SSXHWtfGE03cD/shjZv0o1GrqMH6JdY8jcsGV+pISCZIQG61+lzVSEmXGheuNdt785nzZtM89oySoW7qQnElzWXWq7HWtcA3W7Twst729RO2flnpS/nqzo0/ZOWAXo+TkZDl58qTs3r1bPfbs2ZMvA5XtsQT4Mbd9a4HbUrE/7knomKbbEOICzIkc2p+53IwBA9q7HtR15WK0x1yGcjK8UDkgJAdwuTL48ML9AcqB9lOOCcB9FJPUhz/vkJ02IbpS8QT5ui/cX66XioafdOlCMUpw1YJwpRIJMrV7EyVIYPleuwhACMM4US4pTpYPuU5uu7SclC7svrSPw0zrfpUSDDxRNdl9+dsb5YpkdFF86ibPPvVjf9qSYXndRLuV4Pntuxv61QYIySufu8FyScDki8fXfdNdLPAMwVy7YmjlQI/Rozpfal3Lz3u4+ztrAR3vI9jYm2CDMRsCgBOYAzDOm5ZfWHftxzLnIX3PcUXQLvv9hsJSsXi6woF2wVILyzj20/tAWIU/OLaNd/BD98UTtviIaAe7Ia5PUkJ0BuUA6GkLcx1cZrBygzaZgbAm2k3X7uo7aPIqNZeOmr3ZcrUxFfj/u72O2xwLFxTt/otEH07g/em90q+N6dqB/2Hx1v0FLjn4bZrf6YP7r5Bv+7eUG+uVVitAAy8oB7j/UFBrlUlSx4dQrOUIKEcoMKqFcVwPu3JQo2Qh1Vdn9mnmFgOx/cDJDGnO/U2QYndlNuURrLIh1tEu36DtuFf4nlAO0GZcU/278XY+T+AY19YqaV0bbzh9N/tvxpNLuP494IE2mwpgqSCkHYYb1rJnrlHJY8oWdncVs7uSgfuaVbbku417jyslC21GO0d1buR4DriVb3K41uaq0OApqwNue8AKwsqVK5VigCJpcC1CPEKJEiWkadOmKlA5v2DGEkx+5Cq5c9wSv38MWtvFIDDws5WOfmROS5ZY7oSb0akz5938Lft/ukINLDsOZNQsCSE5C2qHvDZro5vgqJfKYbmEde+Xp69TFkUNChLag+s0LuMY2g8ewgiEP0yi8C02M93sOnbGsmrDSg5LOCZ1CBJaKMRErg0Pfx85LXe9s1QZGjDRmOAwRQrGKquhFoBM4Ov6VZ/mbkKTPVgTFjEtoM4eeLXbvlA87HN/cuLFSRTCyZt3NfLo/w9hDy4WcC+5oU7pDAK3HidNX/ZRc7Y4CtzmZNphws9qPEf7PnuksfXdYQzCdcTj8amrVaFHs0YM2vJFL++KgXk+WAf1sc0xHQK7UwILT/MQ7qe+5xBwIOjYlQCoDkhwASs4at5ooQP7YR+tMMEfHNta1Ui2lA5cvgq2YF1cG/g7m7z8/e9ur88a8xdiCnAOXB/tApJ+nIJWULKOzdBuIfgeOskGmND1YjwAnuH+o+dSbZizC7hvdmrklmADqz+NKxd3TBaC63F19WRlOdfCPsBx9bHRdnwHXFM88D9+g1rBnNq9qYy9+1JLScDxIehri+5tY9Kt6loAxHfU5zDfs8sRaKNul77+M3qlK0gYB6qXvKgI6TTnprHSnwQp9v3MOAWNk3yDc2CMwTm1AuHv+YKBr3OZMpc3A655nBolE91cqfHr/OQh53gUPcaZQDFF/5gzqJWUKJQgl6Qkys6jF1dnMdZ/3/9q9bu4OE5Fyh0Ny7iNY+gP+nrjd+lkrLHHk9i/Pz5vrjz4a9QJ2MXI5MCBAyoGYcaMGfLJJ59IWlqanD+fPunkdRcjDTrags373PzvoNV/3Td9ydJpf9OXTA+QGAT1cpannNlVisdLZGSkbHFYYvr3LbXl3zPXB/OrERIwN9cpKd+s2yOhivZLrlUqUfq3ri7HTp2Tp75Y4+hP7ERMgQg548EvGgLBHwcuTpwQ4Pt9ukIN8LpGiN0lEX6iu46ckisqFrXcKaonF5RBN9SQR22uNdrtApZE+7K56f4AoSYiIjKD6wjOafoY3zxqvpvwhckMllddYwUKBCYbbaV1GuvwHSGE6mNq14g73l4km/elXwtdF8V0T4BB49axi5TVC9lFtth8YysVi7espVAmXr6zvvT/bEWGgo3274Xjmq5Cn3dvoq5X6tnzbi5D5mfsQHiGIK1BYKO29urlfifXHl8uRZ4w24x7DyuhPwqG3UXCcjs17pnGqb3+uH3AreWWMYuURdp0Y9BuJPis7nf2GDcIQHHRBTy2yewv7cYtVn0R53ijUyM31yrLdeTCMYB2m7C71urfldNnvB3Hfg1wXl0jCN9XCeYR6Sv3vtxzTDcku0uO/T6YsoK39+z3Gpj9zJu7mv1376/bjd4PeOtfTjKNPqe/5wsG3s7l5BLu6fevj1PmQmZI9BkI8DDQ4N5//NCVanXqb8OlDMrzjkOnHV3TvLlh6X6B35mOz4CyaY4J9uttvmeOOb76s3nu/91bT0omF/cpOwccpDxt2jQrOHn9+vVSrFgxldnotddeU/EI+Qknn0QMCJ93v8rygcQysdNgpydgc/laD61q2b9wtPxtaJtg2wHPrlxQDvwNyiPEFzDiHs+EK/rqHfuD1gYIGE4CYWaAkPmqEd8TFRGhJgj8Nm+sWzKDb6+neB0oB1oRgGIfHRWpfL0xUH/yUGO5c/wSa1CHKwGsiloIMt/TEy+s0GrQTklU1kQAS0+VlEQryA4CPxQGHbCGCcQOxpbUC1YxKAdTHr1K+Zmbk7rdqhRT4KKfE6xPuBY6ngmTo1NxRWRDsQfMOlng377nckvI1hnV4J6AiQ3XBG3TS+JQDmD80OMbxkbzXuB+wVIPIQ0KCbACiG3fS7sKmRMlrheuj9nuQKyZm2zWXnxHs2Al7v0r7RuoNuE7B+pvbbpi4N4juFMLNbqd/kz8uD4QSpwUFLO9dkuwJ0EJQLmCcgCwumQGqOpzwO3GLiBWLn7Rr99T2/Ea29uqdNsXU3Fr1yotqJmFPu3XwvxeZjCq7sfmeZ22efvu/VAj6MJv0pzj9XVzijnEa3PfTUafsd+HdCHxYj/x9p55zUyh0+n+picwiXD8fdgTpHjCvp9eZbJ/XyeZxr4i48/5goG3czn1E3+OM61nU7e+jc/DzdtUDpDNyq4caNcv+3GnebiG2jg0aekOtY/TOKbx9p6n728/98kT3tPbW5+TAOnevbsqlPbII48ohaBevXqSX7EPBuCtzo2k88SlbtsxIekflx6g8UPCsnCDcoWl4Ytz3I4B/2S7cuAPVA5IsPCmHGA51VM2hL+PeV9BrJEcr4698/ApZSVOiC0g63cdk9goVOcWt9/MJw9daVkvPVnpNahAi+B9k5KFYmXPsVSpXCxBPn24sRJ2tD8tBBEtzCFbh92311u8zrNtaylfbierDaxCOjuHXQixD+oQAPV4AOFTGwu0G4s5oNvP49bWC64JljBzoXCip0ldH8v0LUbKZHt7nSYb5Yft4ZgmZhYRTKB6hcZcLjeJiopUwj9WU8wVChOMqTEF0idvb23wKCjoxXIfi+Zou65Kb67G2AUf8/6oucDmA+2vYGQXXqCcaaHLU/Chxu5OY8Yt2K8JPmu3BPtSlOxtc1I+8Br9XQuIsLaO7pzen+yZcZwEaq0Ya8EK79sVn9fvami5sdiDbJ2ELk/n9fee2NtlZuWx3yOzLdoNSM//dmXUU3t9vecL+2f1dwim5d7pd+Uk02h3qpzA3xWKzFzbcxeOjb5t/gbM1zpD2b3/+dVt1chJufPnGppjhzeFJzOKV6Y+E9DesCjs3Sv5FXtndBoMVACxTWnAoAIt1KmjTfttZ4bzXKg1QkhI8uRNteSV7zY4ZlPwxcZ9p9xcb7RVeNDklVbWFfhHwgIOIddUDjDwIrOLzv6hM9EAKAdwVfn78ClrG5QDgOwesN5DeDd/rwC/xb8OnrSsy76AqxCyophCgV5Kh2uKmV3MtHzahSUn4QuxTHarvT62kzCixyOsVDoJWd4mBSfBL9gTzcj29S0XDe1iZQql2rUSYKUIwj9WKJws804CeiBZ40xjjq4tY7eEm98PVnFfwpZ5HQKxUDpdT7uyYda00TgpHliZ0q6pET6KZZnxBv5eN38FK7N2A1wx/jVmcYa+6kmgdrKam8oz7heUeXOVzV9BKrPpyDO2K92dyuke2e8LlBm794D9mgZT8PP02Zyw3Nv7vZNykF1uRoHe20CurZuLT9mMY7n9N4Hxwsn1yx+yMnZkN5mKQdi6dau899576nnUqFGSkpIi3377rVSoUEHq1LmYHSDUYhCy0lG9TdJmx8Drhi/Mckt9Z02ORkdTfpfjl1jL14SEAwhA/fWZ62T30TPS++PlssXmZ+4Lnb7R9PGG1cl0TwBQGuC+on9zpjtPnVKJMuCGGnL+vCuDn7434HeKids8t/5twlLbqXEFefaLddZ7puCFZ1iLTIXFTH/olB7RWhFwmGACGZM8pVQ2LazaP9r0bfV17JyYuE3s1kVPfrZOdWG8uTb4Ixy4WaRNF4xMpF80j+mrnZnBc3/K6JOO+jjmbwfXuGWNFMlpPLVZ+3p78wH3dX9NzFU2T4VGM+t77vSd/HHtcoxzyEK/CoSc9PHPTBuyUi/KF1m5t95Am+H2ttEwuvh77Mzej5y+j/7KzgG3ZN68ecqtaOnSpSoe4fjx9Iu4atUqGTp0qIQq/kaxe8JuMTBTiJmWAvjWmspB6aRYKzhRWSsPnVKd4I63F1M5ILkK/Oxn9GziNT3kE62ru72GDtBkxFwVpBVp1D6BcvByO3d3Q1j8h99R123bwOurq1zTOlsJnpGZC1l8TPp8skI960xhb3W+1FphWLf7uLLiQznQWVa08qEzOUDwN9FWdfPcUHb0bxOWWigHUHRApWIJVkwQnvHdUOzHzCKC1QJ71hTzu5vZSCAIO4092qrlLRWfWqk0MrfYLZhmVXW4MWnl4I4L58Sz03hnP3ewcLoeTtZFM7WoKezZ22Va/vS18Tdlo/lddV9CUJ/dHSgY80mwrqduq5l+1JNPunbtAXjG62DjKT2krzab1lBvWWacrpun40H5tfcXb2Qlk463+2n2J92WQPtkdsk0/tyvYOLtOmXnNcmOLEk64NrTimx2yZjZMRYHox8E7GKE9Kb/93//JwMHDpRChS4Gy1x77bUyZswYCVW8LQn6o8mZy0AQSmC1gdUxIvKCf+oF7RgWSpNdttzlj/5vmTzepqZKhUhIduOtSjZk40LxMXJtqcIZsuwA9PNpqzO6wGlXAriHmBa92xqWUUFW5rI8fk9PT19rWeKvrFRUKclmbQ/EBoy++1KldGhQaEz/RvGA+44TUMaH3V5XGkHJuBC8Zw8KNoUrvKfP7XRdoOjACgtBS/s7Q/B6Ytoa9d3gNmNVFTWCFU33Bx2LZAUgG4kIfI09Hq1tNv95X77hWNW0u9Q4BTcGij+WLnvApLfMPv4u+9uvDVZkAl2WN93Bsrqk7+98kln8dQeCkoWqzE5VaYNBIBZgb23OjA+4p+Ppe6iDuIPte+4vvlwGs9tVxKkPmiuLObWK4Y3svCbBvreeksjM8BBPkNNjQiAEa+Um4NFkzZo18vHHH2fYDjej/fuDl8EkNzqqU2AULI4QaADSW325apc8/cVaR/9QVdr6Y/eqjXa27T8p3bOhMigh9squsIJ/0auJ7Dt+VtXOgEuQmW3BDKCDFeyR/y1zqwaK4zhlETL9ye1+8/YBG5O4aYlXVcA/W+nYBijc+jcFCzwCAd0CRx3iBNAW/B7tMQHAKdODOQ6Y6FSWpqXbKYMFtjtlTUH6S9NdBu5RZkYgK7jSNvaYArenCcbuP6+3Z5fgk9VJx9fE7cmlxNt3sV8b9LlgBXRm5trllDDojwIFpSC73IoyI/R4anNm/evtn8uq73l2uXNkpzLibx8MJSE1J65JVmI27GQ14LpCCMUSBKsfBKwgFClSRHbt2iWVK1d2275ixQopW7ashDLIVgI8WbPsVY3hq2wKXDoXrpNgAR/pnpOWKwWAkJwE3jQQ1I+cPueWAeaPgyel63+XKWsrBFR7Kja4pQA92dYuWVD2GisOKPSCuhsQyrVSYOaWNy16VltsryEca2uMqtx7Ps0tSNjMnvNF7+ZuucfRZj35q0CwC1lYBny2UmXgMXPlo/14D79tu7VRY6ZNRCak28cuUr9ttVJy4TuhvebnMUHYLfVOE56n1HNmujxffvSerOKeJh5vk6OpUOkUpjk56Xhqm5NwB3wJfE7XICvCQVYFi5wWBnOLUBJ6giH8ZJdPfGYyJWUVpz4YivcrmEJ8bgdc5/aY4K9yG6x+ELCC0KlTJ3niiSdkypQpEhERoYqjLVq0SB577DHp2rWrhCKeJiXzfS142DHjCezKAXyeP3rwCrn97SVuBYcIsVOxaKz8eShVyiSI/OOHDlmrZEE5fOq87DrqLtQ7gS7675m/KwHTKUsP3A+cfMK1MK3fW7/HvQ+jAqneBy47yP+s3Rh0bnnt+uNJ6Tazm+DZboE3BVe8h0w29pLxenLRLgeoXKot8x3GL7G+L36/GJOcgk/tYwBchS5WG05TKxtoi5PwYK4EZFYIcJok/bWKZ9Y9I5BsNf4QjEnHk0+yuc3JHSoUBfJwEXyyQihe96z0w2Bb2DMUqcthtx4nA02o3a+cIBirQsG4dgWycUwI1N0vGP0gYAVh2LBh0qtXLylfvryqmly7dm31fPfdd8szzzwjoYi3QcHMpGGi862bqRTtDP1XbWn71kLZaYszIMQOlAPgj3IAtu0/4VYXwBfo1+ivA2+o7raKUDU5Qc6eT1PKLKzu9sqLiJlxct3RhV60VcpJwTaLBOL4ENztLiMQqv2xwAcy+ZuDMNwAdVYiM6+//XduHwN++/OQ3+OEp2tgtt1pNcApa5Gv7+rNPUO7EASiJATbLz6rkw76gxm3ou+vqdhC0TP7Un4SyEORULvuWemHwbSwO2VaChW3nqyePxSyI/mLrzTQgXyHUOvrWVFug/FdAlYQYmJiZOLEifLcc8+peARkMWrUqJFUq1ZNQhVPgwKUg5tGLXBLXWhV8jxwUioVjZeEuPRCTk7uRXe/+7PHAFBCsoIv5QD9ccqjTeSxqausOIHbxiyU1PMuN6V2676TKuMPtn3bt7myzOrB0yzGZFYtthd68WT1NVcqIJxry68/wrK3gcuXKyDQgz9ihPypIGqOAfh9I25Bp2Y1XXA8CQ++Bmf7+2YObftKhjlp+SvoZGe6wEDIyqRjGmMQ1A23OP0dXunQwApSN/sSIU5kJZ4hWBZ2T9m6/FU6QlUID5WxJi8FaweD3HAfy3TKA6wg4KFBytN///vfsnr1aslptu8/LnUKek4R5TQo6OqjpnJQsVi8PNu2tjz8v/RA4z8uBCcDKAdvdmwgg6asEq0nUDkgucXU7k3l2S/WugURQzkA5oqX/g/bdh89LbXKJGUYVM1ge/DUzTXdfkueBqbqyQUdi6U5uc5kJlON3RXQ036mAqLP7xQAi/3gbqWDjnXGItPX1JPw4GtwNt/3lLXI08Trz7UJteBDfzFXkjDe6kxPeEbyB12sDM8kfMhu4TY7jx8sK3Eg2brsmGMBlGUkOQh2BqrMEm5jTTgEaweD3HAfC6hHTpgwQWbNmqVWEfr16yeNGzeWH3/8UQYNGiSbNm3KtRiEW0YvkoZVSvv0ybJb/My0hODPg6dk1JzNbtZUkwGTV1kCFyFZpWxSbKbc02Dxjo+JymC9cnKRM4E7kc7Xbg6qsVERlnIBMQ0uSqYA62lgwra2by2QPw+dlrpl0l2SvAnTgWaq8TSo+1JAvGU8sQcC2wPRPLny+Bqc7bEKTlmLsjJpBct6lJNWS7sQZI63WKWyV532FlQdqtbW/Ii/FuasFI0KVeE5WAKbORbgd4GVNSQ5CIW+HYqBznkhWDsY5LQLlN+/uhEjRii3ovr168uGDRtkxowZKuZg9OjRSll49NFHpWjRopJbBDLhYgDy5HuN5e0J91wqj36UMRUplQMSSM0BXYHXCbw3o3dTuXviLz4D3HWmLNNKhT6sswNBwC9XNF62XsigBdeZr3o3U4G3/T9doQqMYRtcjezBt2YaTzHa6xQgbBe8O09cqpSDi9/I+4pdZjLVOBHI4B9oekxv7fQ1OJvvO50jK5NWMKxHOe06YBeC9MoKnt+8q6FVT0LfF09B1eHm8pDX8UfRzco98yU8h5KymFmBDW03lWb8LkLFyh2Ogc55MVj7XAj0c78VhPfee0/FHnTr1k0WLFggLVu2lMWLF8uWLVukYMGsp9DLKt7yi3uabBAY92WvZtJ+/GK3+IK35mxWqSNtdaNIHuf2BiVlxqo9mVIEyxSOlX+OXlwNwCrUJw9fKd+u2SvvLtymVqdMcI6jp9Pkm35Xq9Sefx44odJ7pl5wD9L+8fZaAwD9Ggquzg4E679WDsD0ns1UlqFq8TEyo3fzDLn8zeBbWNDtmY/s9QmcwDHsMQjeFAq7UIE2OVnv/RnUAxn8A02PGbT80Q7nyOqklVXrUU4vu9uvvb0fm+/Z08vmZruJd/xRdLO6WuZJeM4ryiLa/HmPJnLZS3NU+mgYe8y6L7lNXgh0DuWAY1+ESj/3W0HYsWOHqpYMWrRoIdHR0fL888+HhHIws08zqVOxlJuFwdPFNQcuCDioXWAPPobFleQ/vli1J9OfNZUDABc1c3VAB7mbRcb0xKor7tpjDPYfT7XSitonRwj1F10y3INzUaRLo5UATxM63p/eq5nM3bhXrTA41SdwAscwFQtfCoU9SBgKi9PA5++gHsh+gQjl2b00nZuTVk4vuztde/O7O7pkOfSJvOouECoEKsh5WiEMlouHU+HBYLjphRqou6Kr18PY42+sVjgQKgJuuP7WdoRIP/dbQUhNTZW4uIsaLuIQihUrlqWTz58/X1555RVZvny5Kr42ffp0uf322wM+TuUS7gHKni4ubg4qymohDfxtVI4l+YsCESIeMthmGawgmK5DUA4qF0+Qmb2bWUXGwILN+zIoBxD4n5y2JkNebbty+12/FqpugKfg3ECE5FGzNxvn915YSw9yU7o3saqM+wrOc3JpwnfJiWw1pmDqa4DOC0vTIRXk5kUh0u+h2ra3yTAv35NwFeTsvylv9UMyc888FR7MS8pibnyXnLLqh4qAG66/tQoh0s8DivwZMmSIJCSkN/TMmTPyf//3f5KUlJ4VRfP666/7fbwTJ05IgwYN5IEHHpB27dpJsHC6uLg5Zt52kj/jAkyyqhzEREHwd35vdOdLZfCUVW79DRmzoBxoZdXeH1FL4I27Gqr/dW5/c3CFpbW6oXgMnrpapjx6lTXgZzYnst1dSFdYhuAWTH9wJ5cmBKrC9zwnhD5/2x7OS9O+CMXv5s9kGIrtzgsEQ5DzVj8kq9Wqg+2mF0rYv4unMTccrfqhIuCG62+tQIj0c78VhKuvvlo2btxovW7atKls27bNbR9UMQ2Em266ST2C0fHNH5bTxcX7VA7yFzqTj14xiouOlNNn0yS2QITy9Y8rEOlWKdsX9rgUu3Kgz4P4gXJF4lQRrzPn0mTA5JUqyNi+VG7vj1AOdB0BJwUXbhjmqsQaL/n2A7Ek2Qdz1Bbwx0UvMwIFjmMWN1uTg9alvGbVym0f32ARKpNhfiQYglxuuK6F8+/W6bvkhPCek+NfTv6mw2UcrBDg7yQU+rnfCsLcuXMlt4GbEx6ao0fTBawuE5fK7wfPZUjNaF5c3AxPedtJ+K4CePxMVISV5hNC+7Db68qkpX+q+BKtxlZNLiivdky32A+avNJr7IkW/vUz3IBcrovVe83UuAgubjd+SbpSUDZJBcLbC4U5+fBrtx5PCq7dFclTvn1fA6jTZGSez9tE4qkSbiDgewYyUGZlAvCmCIWTVcufe+h0bcJl8gyFyTA/EgxBjgpeeAjveVGRC6dYhwJh+DsJveTCXhg+fLgKjLazbtdRiYxNsHyaUXQHgoxdKIMweOvYRbnQcpKVOIHMFKQ7e95lCbK6cq5GFxJDgTBdoOnMefeTaEUAbkSjOzWSRyetUNuxTRfXAuhvAFmFdCVYAOXAU55+MzhYf97uw++k4OrBHTUHBt1QQ66oWFTufvcXrwO+0wDqaTLS5/M0kehVDKdKuNk1UGZlAvClCIXDAA3swelYgQHZmWoyPxIuylQoCnJU8EJfeA9HAdUX9rks1CuwFwiz30lYKQhPPfWUDBw40G0FAdWc65QurFYQYNXVxXcsS2+ZwvJKhwbKX9te80CTFBspR1KZ0zQUyWycANx8Pu/eRJbvOOyW5z/9vfT6AWZ/MUERJ7gTrd99XLkRjf5pq5UxCIM3MgvpQVYPRhAu9IoABPiIiEhrf0+DPQYLfwczPbhjAESbdTEzs4qw04DvpAz4mow8TST2/ORZybrh70Dpqf1ZyQQRTgM0sAenwz3LzGKVHakm8xtUpkhuklPCe7gJqL6wr8TnZExbfiCsFITY2Fj1sDPp4cZy+FwBZcXVvs06SxE6jmnZdYLKQc4Atx9Y9oOFVgIhkB86eVZ2Ghmp4OaDgGAzxadONXpJiQR5o1MjtZ/uLxp7EScAN6L/drtcyhdLUP75TikZ9YqAGXAW7MEex8GKh1Zo9OqEN4HZUx0AX5OR00SSGy469nNiZTDcMkFk1Tptfg8NxrVZA65W/cHTMUPp+4c6VKZIbpPXhPecIDdj2vIDYaUgeP1hFU2U02fOqVSSyBbjiSi4g+Ro64jGVA7KFomXnYfdi4f5S8Wi8TK2SyOpmlJICcimYmhmBNKCE6zsN41aYPUL7VpkClBmlWJgWiWw4vDAB8uUW82bnRp6FCTsA3x2DFKBCsyelIHMTEa5lSbT3/iIUGhvdlin7atHetXA39Syuf39w8G1h8oUIeFJoDFtxH8iXC6EWuYOx48fV5WYQaNGjVSK1GuuuUbVV6hQoYLPz8PFCGlWDxw8JAfORFmTp7YUVywaJ38eYp2DUAOW/+XPXCe7jqZK/09XWMHBTlmFEEi8Zd8JtUow7I760mHCEqvy5IohrVW+bFMAq1QsXsbc3Uhqlk6yhA0E+F772jzrmDVKFpKv+za3gj09VdyGQIYKx2YBMaQZjY0uYAlp03rk/HKm2WY8m9/tx0Et85T1xGtwbi5d/0Cx97+s3KNQEKbzqmtPXru2hOQX+NsNDC07HzlyRAoXRqHVIK4gLFiwQCZMmCBbt26VqVOnStmyZeV///ufVK5cWZo3b+73cZYtW6YUAo2OL+jWrZu8//77fh9HZzHSQDlISYyWVzs0kA7vLPX7OCRngIAP9x/437/ZqZFl/YdyMPHey+Tl736XLftOSp3SiTKyfUPZe+y0NKlSXJZuP+hWeXLx1gNyba2SPi2lpnUQLkQzejX1aUnHdlgmBn620m070oz6cu3Ibsw252XLZ3YUYMoNAr1H3ia7vOaGEEquPXnt2hKSX+BvN3sIWEH4/PPP5d5775UuXbrIihUrrLSj0ESGDRsm33zzjd/HatWqlQRjAUNnMTLZe/ysdKRyEJKYQpK5PAg3n0olCspXfVrI1n0npOek5dJ2dHosAAIyP3noSrdVhtdnbZKrqyc7prU1yaxQ6VSrwB/XjpwkHAXm3CzAlBuWppzK2hSO5GUFlxBCwpmAZx5UTx4/frxMnDhRoqOjre3NmjWT3377TXIDZDHS+fJNcs13Kp9QLbmg1C6V6JY5SLvheALBvq91SK/UqxnZvr682/UyVVcAqwntxy2WduMWy58HL8YowKUHLklwHzKDhyF0+QOELO2SAyEsEOEFQHnBykEourRogTnU2pVVzOtvFx51cUR/76UpfMPdB8+BfDan7pGTUpSX0coT3K5C8bdFCCH5lYBXEFBNGVWV7cCf6fDhw5IbIIvRqj2pKvUjCQ7I9LN1v3fhZPO+EzKwdTVZv3uzlTno5Xb1pG29UtL53V+UUJ9cMFqSC8fJ+l3HlJD9xuzN6f77F1J0dhi/JIOVHkHEduqULmTFmOg0pYG4ayCY1yn7UH61zocDnq5/Zq3soeTO4on8aFHPy+4B9I0mhOQbBaFUqVIqsLhSpUpu2xcuXChVqlSR3GDznmMydMamXDl3OFOpeIKqNqzTj8YViFBFxCCMv9axoZw6c17uGLfYY3pRlBh7ffZm9ey6sILwxLQ18tHSHTLFyM8PMEma2YYgACGmwK4cAMQeIAZBxxuAx9rUsBRAKAe6WJm/7hrIQITc/YEKh3lZeAkHAcrp+mdW0A8H4ZtKad4hv7mLhQtU2gjxj4B/HQ8//LD069dPli5dKhEREfLPP//IpEmT5LHHHpMePXpIbtBhws+yg9mKAgJCPpSDysUSrPSjUA7gAhQZGalqRzw3Y62bCxFAhqilT10jw++oa7lw4RlB4VhBALDy/3XolOXSAyDA6XgDoAuOIZ2o3UUpMjJKnQMpa/W+CFI2P+tLOXAq7IUAZf353BAOM+MWk5fJrMuPN9ejvODOklddxvIb+c1dLBzITTdDQvL8CsKTTz4paWlpct1118nJkyeVuxGKl0FB6NOnT/a0kjgC9519J876vb+29ANtnd9+8KSKGUB2HrgAoRiYLsTlZN1Hhqh9x8/KR0v+yBAUboLVAn8y0KC4GNKJ/n3opLVCgPMfPHlOZg1s6VZ47LWO6bEL/gYJ2y3Gkx/xXnU4OwlXa2J2WtsyuxKQFSs7V4RIThEOK1b5jXBwMyQkbBUErBo888wzMnjwYOVqhFoGtWvXlsRE/shymkCUg9KFY2TX0TMZ3IugFFxMJBWhKgWbWYWwze73D9btTnfXMQOW/z58Sq0iYCUA2anMgRhKgE4Nag7IENiQ7tSp2IkW5pyEa39wEiRzazIIx4kpu5WarAhQFPRJqEN3sdCDShshOVBJOSYmRikGoUCBCBEuFDqDAmOw+m/a4y7Qv9WpkSTGFXCLC4AiAAu7OakBHeRrxhSYlYarpyTKqE4N5aa30lOSQknAZIjUpDgmqhpbFWA9CJq64jHiEuB6ZL6fFeE6VATJcJyYslupoQBF8jqhMv6QdDjmEBJkBaFdu3Z+H3DatGmS05xzZSKYIoyJjRJJPZ/+f0yUSOmkBPnTwb8Vgvsr7RtYgrvJM1+slekXLPGeLPca/b+5bUr3JqpQWemkOLUCYD8OViL00sSZc+eVC5M3QRPWak9ZhtAmrWzgORyE67wwMeWEUkMBihCSk3DMISSICgJSmGrgOjJ9+nS17fLLL1fbli9frlKcBqJIEHcKFhA5cbEYtEfgyhMV4ZINe9MVAleayKvt60uX/yxVKwVYTYHCBDbtPa4GQy3k6VgDsMYQ1AMVXD0J8+ZxzCJjOCcChDfuOe5R0PRprdZ+UEEorJdbhNvEFI5KDSGEEEJySEF47733rP+feOIJ6dixoyqWFhUVpbadP39eevbsKYULX8xIQwLDH+VA1x4wOesS6fDOz9ZrrRzooOTShWMtIc+tFoAhqAcquPpT4dafAGEzANabtdpUNvAcDv77eYVwU2oIIYQQknUiXFgSCIDk5GRV86BGjRoZCqg1bdpUDhw4IDnF0aNH1UpG+f6TJTI2/NxOMkP5InHy12H/U7qiXkDLGilBzUrjFrxaLsljykhv57IHwCL+AKlRnbIU+Xs+QgghhBDiW3Y+cuSIV8N+wEHK586dkw0bNmRQELAN6U9J9qBTlAaiHCCbEAJ+s2oRtgv6/rqeeDuXfRXitrGL0l2QHLIU0dWFEEIIISTnCFhBuP/+++XBBx+UrVu3ypVXXqm2oWjaiBEj1HskcyDY+MyFwGPQ79qqMurHLdZrb8s8w26vK5dWLCq7jpyyagmA6T2bSVxMphNVeU11mVXXE9OlSMcneAtipqsLIYQQQkjOELD0+Oqrr0qpUqXktddek127dqltpUuXVnURBg0aJPkdRGUYcr5PINzDbeaJz1fLul3H1DZk6nm4RWV5Z8F2VXsgNipCLSGkmgEGF0B9gnaXllWKQNWURDc/frwO1VSX5qqAp9gIQgghhBASBjEIdj8mkFvByaEegxAdKXLWi9cVhPsVQ1qr4F2UftfMGnC1Sh16+sw5qy4ALPm3jFkk2w+kVz5G+tKDJ89IkyrF3VYJgl35Nqf8/7OzYi8hhBBCCJHsi0HQ7Nu3TwUmg5o1a0qJEiUye6g8iek2A56/pbZ8tHSHbN57cRtWB3RmH9PyjyBdAMHfDDCeNbBllvz+M4O//v9ZFfDpQkQIIYQQEhoErCCcOHFC+vTpIx9++KEVlIx0p127dpXRo0dLQkLoWfJzmhdvrSO3NCgld72zVCkJcQUiZejM9RlWFVDIDO41wQj6zU58nddTnAIhhBBCCAk/ApbiBg4cKPPmzZOZM2eq4mh4zJgxQ21jDEI6Q75cJ42H/6SUg8rFEuT0uYt+RqbLEQqZwfceArYWwsNRsHaKUyCEEEIIIeFJwNLo559/Lv/5z3/kpptuUr5LeNx8880yceJEmTp1ava0MgxJvaAUbD94UioVi/e4X14QqLWLFGCQMSGEEEJIPnMxOnnypJQsWTLD9pSUFPVefqVAhHsVY01sgUj5oldT6TB+iWzel359qiUXlJgCkSprUb2yhb0K1OEQvMs6BYQQQggheYeAJbkmTZrI0KFD5fTpiwW7Tp06Jc8//7x6Ly/zwX2XqoJlTkA5qJAUK6UKxWRYSTh48px827+lyk6Ex8w+zSUyUl/6CJ++/chwhGe8DlXC2UWKEEIIIYRkYQVh1KhR0qZNGylXrpw0aNBAbVu1apXExcXJ999/L3mZeZsOei1YtuNIaoZtqGmgrepIXQq27Tsuay747K/xUlsgu2oQEEIIIYQQEjQFoW7durJ582aZNGmSbNiwQW3r3LmzdOnSReLjPfva5wUeaVlRPvn1L5We1FeNA/Bu18ukVY2UDFZ1e1pTTy5G/u5HCCGEEEJISBRKy21yulDaj4NaqrSkKF7WoFxh6TDhZ9m894QVawB3IqQ0RdYixBZM79ksy3UDwiEGgRBCCCGE5J1CaQFLnB988IF8/fXX1uvHH39cihQpIk2bNpU///xT8goVisa5vUbNgvNpLiWkN6taQrr+d5mlHAAoBx/cf4WsfO56pUh4Uw4C8dmnbz8hhBBCCMlJApY6hw0bZrkSLVmyRMaMGSMjR45UlZQHDBggeYUdh05LPPyIjKxD178xXwULb99/wooN0MAFCIoDqh9ToCeEEEIIIfkmBuGvv/6SqlWrqv+/+OILad++vTzyyCPSrFkzadWqleQlTl0IMoiIEFn7z1H1PxQDrCTUKJmoCqHBlej1jg2lcomCVAoIIYQQQkjYE7BEm5iYKAcOHFD///DDD3L99der/5HFCOlOwx0EFiPzkMmmvSeUQgCgEAyeulopBzVSEmXKo01UdiIqB4QQQgghJF+uIEAheOihh6RRo0ayadMmVUUZrFu3TipVqiThTHx0lDSvWkJlHoIb0cDJq1QaUrgPTX7kKvnnyGm1egBXI7Bx73G1jalHCSGEEEJIvlUQxo4dK88++6xyNfr888+lePHiavvy5ctVutNwBulLtcCPVYHpturA2I6sQkw9SgghhBBC8ipMc6pWDiJVvAEE/mk9mvp0F2LqUUIIIYQQklfTnPq1grB69WpVIC0yMlL974369etLuAHlAClKkYUIAr8vBUCvJhBCCCGEEJLX8EtBaNiwoezevVtSUlLU/xEREWIuPOjXeD5//ryEY+xB48rFLOUAqUyVC1HZJJnW0/eKAiGEEEIIIflKQdi+fbskJydb/+c1zNgDrBzoGgd4xmuuFhBCCCGEkPyCXwpCxYoVHf/PK5jBxng2g5DLJMXJtn3HGW9ACCGEEELyBQFnMQIbN26U0aNHy++//65e16pVS/r06SM1atTIVCOQGemVV15RbkwNGjRQx77yyisl2ERFIH4gQlLPpbtH1SldSN7s1MityBme4VaElQMoBx0n/Ex3I0IIIYQQkm8IWEFAatNOnTrJ5ZdfLk2aNFHbfv75ZxXE/Omnn8qdd94Z0PE+++wzGThwoIwfP14aN24sb775prRp00YpIYh5CCbnXSLjO18qUZERUjopzmOBMx2EjJUDuhsRQgghhJD8RMBpTi+55BLp0qWLvPDCC27bhw4dKh999JFs3bo1oAZAKbjiiitkzJgx6nVaWpqUL19erUg8+eSTQU1zGlsgUlY9d73ExfinF7kFLPuZApUQQgghhJBwTnMasLS7a9cu6dq1a4bt99xzj3ovEM6cOaMKrLVu3fpigyIj1eslS5ZIsClfND4gAV+7G/04qCWVA0IIIYQQki8IWOJt1aqVLFiwIMP2hQsXSosWLQI61v79+1Va1JIlS7ptx2vEI9hJTU1Vmo/5CIQt+04oN6FA0O5GVA4IIYQQQkh+IOAYhFtvvVWeeOIJZfm/6qqrrBiEKVOmyPPPPy9ffvml277BZPjw4eoc/lAtuaDEREfJun+OKtei1HNpbtmKCCGEEEIIIUGIQYALkD/4UzQNLkYJCQkydepUuf32263t3bp1k8OHD8uMGTMyrCDgocEKAuIVqgyaIucLxEulYvEyqlNDSYiNVpmJgM5GhDoHTFVKCCGEEELyK0f9jEEIeAUBQcTBIiYmRi677DKZM2eOpSDg+Hjdu3fvDPvHxsaqh50lT14rx10xjgqAzjrE7EOEEEIIIYRkUx2EYIIUp1gxQNpU1D5AmtMTJ07I/fff7/cxkJUopTAVAEIIIYQQQnJMQbj55pvlk08+UcsSYMSIEdK9e3cpUqSIen3gwAEVpLx+/fqAGnDXXXfJvn375LnnnlOByQ0bNpTvvvsuQ+CyE9o7KtBgZUIIIYQQQvIbRy/IzL4iDPyOQYiKilJpTHXxMvgtrVy5UqpUqaJe79mzR8qUKeMz7iCYbNu2TdVlIIQQQgghhPjHX3/9JeXKlcv6CoJdjwgwtjlbKFasmHresWOHtbJBiL/oIHf8SLwF6hDiBPsPySzsOyQrsP+QrAD5/dixY8qoH9IxCFlBZ1SCcsAfCcks6DvsPySzsP+QzMK+Q7IC+w/JLP4Y1f3O+Ym0pXjYtxFCCCGEEELyDgG5GN13331WmtHTp0+rIOWCBdPrDZj1CQghhBBCCCF5XEFAKlKTe+65J8M+Xbt2lZwEysrQoUMdayMQ4gv2H5IV2H9IZmHfIVmB/YeEZCVlQgghhBBCSN7F7xgEQgghhBBCSN6HCgIhhBBCCCHEggoCIYQQQgghJG8oCGPHjpVKlSpJXFycNG7cWH755ZfcbhIJIsOHD5crrrhCChUqpCp433777bJx40a3fZBNq1evXlK8eHFJTEyUO++8U1X1NkEhvbZt20pCQoI6zuDBg+XcuXNu+8ydO1cuvfRSFfRVtWpVef/99wPub/60heQOI0aMUGmZ+/fvb21j3yHe2Llzp0rGgXsSHx8v9erVk2XLllnvI3zvueeek9KlS6v3W7duLZs3b3Y7xsGDB6VLly4qV32RIkXkwQcflOPHj7vts3r1amnRooXqGyh+NXLkyAxtmTJlitSsWVPtg3Z88803bu/70xaSc5w/f16GDBkilStXVvfjkksukRdffNGtwCz7Dwl5XGHKp59+6oqJiXH997//da1bt8718MMPu4oUKeLas2dPbjeNBIk2bdq43nvvPdfatWtdK1eudN18882uChUquI4fP27t0717d1f58uVdc+bMcS1btsx11VVXuZo2bWq9f+7cOVfdunVdrVu3dq1YscL1zTffuEqUKOF66qmnrH22bdvmSkhIcA0cONC1fv161+jRo11RUVGu7777LqD+5qstJHf45ZdfXJUqVXLVr1/f1a9fP2s7+w7xxMGDB10VK1Z03Xfffa6lS5eq+/z999+7tmzZYu0zYsQIV1JSkuuLL75wrVq1ynXrrbe6Kleu7Dp16pS1z4033uhq0KCB6+eff3YtWLDAVbVqVVfnzp2t948cOeIqWbKkq0uXLmqc++STT1zx8fGuCRMmWPssWrRI9amRI0eqPvbss8+6oqOjXWvWrAmoLSTneOmll1zFixd3ffXVV67t27e7pkyZ4kpMTHSNGjXK2of9h4Q6YasgXHnlla5evXpZr8+fP+8qU6aMa/jw4bnaLpJ97N27F+YX17x589Trw4cPq4EOg6/m999/V/ssWbJEvYZQFxkZ6dq9e7e1z7hx41yFCxd2paamqtePP/64q06dOm7nuuuuu5SC4m9/86ctJOc5duyYq1q1aq5Zs2a5WrZsaSkI7DvEG0888YSrefPmHt9PS0tzlSpVyvXKK69Y23AfY2NjlZAGIIzhHv7666/WPt9++60rIiLCtXPnTvX67bffdhUtWtTqT/rcNWrUsF537NjR1bZtW7fzN27c2PXoo4/63RaSs+B+PfDAA27b2rVrpwR5wP5DwoGwdDE6c+aMLF++XC2DaSIjI9XrJUuW5GrbSPZx5MgR9VysWDH1jD5w9uxZt36AZdQKFSpY/QDPWFItWbKktU+bNm3k6NGjsm7dOmsf8xh6H30Mf/qbP20hOQ/cduAiZL+/7DvEG19++aVcfvnl0qFDB+Va1qhRI5k4caL1/vbt22X37t1u9ywpKUm5j5n9B24hOI4G++P+L1261Nrn6quvlpiYGLf+A1fKQ4cO+dXH/GkLyVmaNm0qc+bMkU2bNqnXq1atkoULF8pNN92kXrP/kDxVKC2U2L9/v/LxMydugNcbNmzItXaR7CMtLU35jzdr1kzq1q2rtmFQw8CIQdTeD/Ce3sepn+j3vO0DQfDUqVNqoPXV3/xpC8lZPv30U/ntt9/k119/zfAe+w7xxrZt22TcuHEycOBAefrpp1Uf6tu3r7pPKBqq74vTfTX7BpQLkwIFCigDh7kP/NTtx9DvFS1a1GMfM4/hqy0kZ3nyySfVGABFPyoqSo0BL730koonAOw/JBwISwWB5E9L8Nq1a5UVhhBf/PXXX9KvXz+ZNWuWCswjJFCDBCy3w4YNU6+xgoDxZ/z48UpBIMQbkydPlkmTJsnHH38sderUkZUrVyoDV5kyZdh/SNgQli5GJUqUUFq5PcsHXpcqVSrX2kWyh969e8tXX30lP/30k5QrV87ajnsNF47Dhw977Ad4duon+j1v+yBzBDI6+NPf/GkLyTngtrN3716VXQhWNzzmzZsnb731lvof1jH2HeIJZHOpXbu227ZatWqprFZA3xdf9xV90AQZsJCZJhh9zHzfV1tIzoJsZ1hF6NSpk3JTvPfee2XAgAEqMx9g/yHhQFgqCFjmveyyy5SPn2nxwesmTZrkattI8EAQPZSD6dOny48//phhKRV9IDo62q0fwPcSk7juB3hes2aN20ALqzIEOC0AYB/zGHoffQx/+ps/bSE5x3XXXafuOyx3+gGLMJb49f/sO8QTcGW0p1SGP3nFihXV/xiLIDyZ9wwuJfANN/sPlD4oqxqMY7j/8O/W+8yfP1/FoJj9p0aNGso9xJ8+5k9bSM5y8uRJFStgAkMB7j1g/yFhgStMQepARNm///77Ktr/kUceUakDzYwjJLzp0aOHSr02d+5c165du6zHyZMn3dJDIvXpjz/+qNJDNmnSRD3sqSpvuOEGlSoV6SeTk5MdU1UOHjxYZY8ZO3asY6pKX/3NV1tI7mJmMQLsO8RbatwCBQqodJWbN292TZo0Sd3njz76yC01JO7jjBkzXKtXr3bddtttjmkqGzVqpFKlLly4UGXUMtNUIlsM0lTee++9Kk0l+grOY09Tiba8+uqrqo8NHTrUMU2lr7aQnKNbt26usmXLWmlOp02bplIkI+uZhv2HhDphqyAA5BzHpIoc40gliFzBJO8A/dXpgdoIGgxgPXv2VKneMDDecccdSokw+eOPP1w33XSTyg+NQXrQoEGus2fPuu3z008/uRo2bKj6UpUqVdzO4W9/86ctJHQUBPYd4o2ZM2cqBRHKXc2aNV3vvPOO2/tIDzlkyBAloGGf6667zrVx40a3fQ4cOKAEOuTAR3rc+++/X6XeNUHeeaRUxTEgVEJYszN58mRX9erVVf9BWt2vv/464LaQnOPo0aNqrMFvPi4uTo0LzzzzjFs6UvYfEupE4E9ur2IQQgghhBBCQoOwjEEghBBCCCGEZA9UEAghhBBCCCEWVBAIIYQQQgghFlQQCCGEEEIIIRZUEAghhBBCCCEWVBAIIYQQQgghFlQQCCGEEEIIIRZUEAghhBBCCCEWVBAIISQfMHfuXImIiJDDhw/ndlMIIYSEOFQQCCEkD9KqVSvp37+/9bpp06aya9cuSUpKyrU2UUkhhJDwoEBuN4AQQkj2ExMTI6VKlcrtZhBCCAkDuIJACCF5jPvuu0/mzZsno0aNUhZ7PN5//3036z1eFylSRL766iupUaOGJCQkSPv27eXkyZPywQcfSKVKlaRo0aLSt29fOX/+vHXs1NRUeeyxx6Rs2bJSsGBBady4sVoZ0Pz5559yyy23qM/i/Tp16sg333wjf/zxh1xzzTVqH7yHtqCdIC0tTYYPHy6VK1eW+Ph4adCggUydOjXDysPXX38t9evXl7i4OLnqqqtk7dq1Ps9LCCEkcLiCQAgheQwoBps2bZK6devKCy+8oLatW7cuw35QBt566y359NNP5dixY9KuXTu54447lOIA4Xrbtm1y5513SrNmzeSuu+5Sn+ndu7esX79efaZMmTIyffp0ufHGG2XNmjVSrVo16dWrl5w5c0bmz5+vBHXsm5iYKOXLl5fPP/9cHW/jxo1SuHBhpQwAKAcfffSRjB8/Xh0Dn73nnnskOTlZWrZsabV38ODB6rthJeTpp59WCgG+Z3R0tMfzEkIICRwqCIQQksdAnAFcirAqoN2KNmzYkGG/s2fPyrhx4+SSSy5Rr7GC8L///U/27NmjhOvatWsrq/9PP/2kFIQdO3bIe++9p56hHACsJnz33Xdq+7Bhw9R7UALq1aun3q9SpYp1vmLFiqnnlJQUpYToFQl8bvbs2dKkSRPrMwsXLpQJEya4KQhDhw6V66+/Xv2PVY5y5copBaVjx45ez0sIISQwqCAQQkg+BQqEVg5AyZIllWuRaXnHtr1796r/sUoAd6Pq1au7HQdCfvHixdX/cEnq0aOH/PDDD9K6dWsltMMtyBNbtmxRKxla8NdgNaBRo0Zu27QCoZUNuEb9/vvvmTovIYQQz1BBIISQfApcc0zg5++0DTEC4Pjx4xIVFSXLly9XzyZaqXjooYekTZs2Kl4Awjrch1577TXp06ePYxtwTID9EddgEhsb6/d3CfS8hBBCPMMgZUIIyYPAxcgMLg4GsOjjmFhRqFq1qtvDzJCEeIPu3bvLtGnTZNCgQTJx4kSrTcBsF9yYoAjARch+TBzH5Oeff7b+P3TokIo/qFWrls/zEkIICQyuIBBCSB4ErkJLly5V2YNg3derAFkBrkVdunSRrl27Kus8FIZ9+/bJnDlzlDtP27ZtVe2Fm266Se0LIR7xC1qIr1ixolqRQOakm2++WQUpFypUSMUxDBgwQLWxefPmcuTIEVm0aJEKZO7WrZt1fgRcw5UJbk/PPPOMlChRQm6//Xb1nrfzEkIICQyuIBBCSB4EQjfcgGChRzYgWOiDAYKRoSDAQo8YAAjov/76q1SoUMFaHUBGIQjnyG4Egf3tt99W78GF6Pnnn5cnn3xSCfnIiARefPFFGTJkiHIL0p+DqxDSnpqMGDFC+vXrJ5dddpns3r1bZs6c6bYq4em8hBBCAiPC5XK5AvwMIYQQkmOgDgKyKWFlQGc/IoQQkn1wBYEQQgghhBBiQQWBEEIIIYQQYkEXI0IIIYQQQogFVxAIIYQQQgghFlQQCCGEEEIIIRZUEAghhBBCCCEWVBAIIYQQQgghFlQQCCGEEEIIIRZUEAghhBBCCCEWVBAIIYQQQgghFlQQCCGEEEIIIRZUEAghhBBCCCGi+X9/bZ2wK/uNEAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], ts, results_plotter.X_TIMESTEPS, \"CantileverEnv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b078e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32004866",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f63109a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = PPO.load(log_dir + \"best_model.zip\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a4d23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "ans=[]\n",
    "while i<1000:\n",
    "    action, _states = model_best.predict(obs)\n",
    "    obs, rewards, dones,_, info = env.step(action)\n",
    "    ans.append(obs)\n",
    "    if dones:\n",
    "        break\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7568253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Cantilever beam design:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(ans[-1]['macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb3d5c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Cantilever beam design:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(ans[-1]['micro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed8ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
